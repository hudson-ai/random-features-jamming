{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext line_profiler\n",
    "%load_ext autoreload\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.keras.backend.set_floatx('float64')\n",
    "from numba import jit\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "from tqdm import notebook as tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from https://www.openml.org/d/554\n",
    "from sklearn.datasets import fetch_openml\n",
    "X_raw, y_raw = fetch_openml('mnist_784', version=1, return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = 5000 #train\n",
    "P_test = P\n",
    "\n",
    "X = X_raw[:P+P_test]\n",
    "y = (2*(y_raw.astype(int) % 2) - 1)[:P+P_test].reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=P_test, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "d = 20\n",
    "pca = PCA(n_components = d)\n",
    "pca = pca.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pca.transform(X_train)\n",
    "X_test = pca.transform(X_test)\n",
    "\n",
    "# project to hyper-sphere of radius sqrt(d)\n",
    "X_train = np.sqrt(d) * X_train / np.linalg.norm(X_train, axis = 1, keepdims=True)\n",
    "X_test = np.sqrt(d) * X_test / np.linalg.norm(X_test, axis = 1, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = int(1.1*P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonlinear_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.InputLayer(d),\n",
    "    tf.keras.layers.Dense(N, use_bias=False),\n",
    "    tf.keras.layers.Activation('tanh')\n",
    "])\n",
    "\n",
    "linear_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.InputLayer(N),\n",
    "    tf.keras.layers.Dense(1, use_bias=False, kernel_initializer='glorot_normal'),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = nonlinear_model(X_train)\n",
    "test_features = nonlinear_model(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC as SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SVM(penalty='l2', loss='squared_hinge', fit_intercept=False, dual=False, C=1e15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = svm.fit(train_features, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = svm.decision_function(train_features)\n",
    "acc = svm.score(train_features, y_train)\n",
    "plt.hist(p, bins = 100)\n",
    "plt.title(f'Train: {acc*100:.0f}% accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = svm.decision_function(test_features)\n",
    "acc = svm.score(test_features, y_test)\n",
    "plt.hist(p, bins = 100)\n",
    "plt.title(f'Test: {acc*100:.0f}% accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = min(1024, P//2)\n",
    "\n",
    "#Initialize with SVM weights\n",
    "# linear_model.layers[0].set_weights([svm.coef_.T])\n",
    "\n",
    "#Initialize *near* SVM weights\n",
    "linear_model.layers[0].set_weights([svm.coef_.T + np.random.randn(*svm.coef_.T.shape)/np.sqrt(N)])\n",
    "\n",
    "linear_model.compile(loss='squared_hinge', optimizer='adam')\n",
    "train_data = (\n",
    "    tf.data.Dataset.from_tensor_slices(\n",
    "        (train_features, y_train.reshape(-1,1).astype(float))\n",
    "    )\n",
    "    .shuffle(buffer_size=min(5*batch_size,P))\n",
    "    .repeat()\n",
    "    .batch(batch_size, drop_remainder=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 500\n",
    "\n",
    "losses = []\n",
    "for batch in tqdm.tqdm(train_data.take(n_steps), total=n_steps):\n",
    "    out = linear_model.train_step(batch)\n",
    "    losses.append(out['loss'].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)\n",
    "plt.xscale('log')\n",
    "# plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = linear_model(train_features).numpy().ravel()\n",
    "acc = np.mean(p*y_train > 0)\n",
    "plt.hist(p, bins = 100)\n",
    "plt.title(f'Train: {acc*100:.0f}% accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = linear_model(test_features).numpy().ravel()\n",
    "acc = np.mean(p*y_test > 0)\n",
    "plt.hist(p, bins = 100)\n",
    "plt.title(f'Test: {acc*100:.0f}% accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(svm.coef_.T.ravel(), svm.coef_.T.ravel() - linear_model.layers[0].get_weights()[0].ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "Ns = np.unique(np.logspace(0, np.log10(1.1*P)).astype(int))\n",
    "for N in tqdm.tqdm(Ns):\n",
    "    N = int(N)\n",
    "    \n",
    "    nonlinear_model = tf.keras.Sequential([\n",
    "        tf.keras.layers.InputLayer(d),\n",
    "        tf.keras.layers.Dense(N, use_bias=False),\n",
    "        tf.keras.layers.Activation('tanh')\n",
    "    ])\n",
    "\n",
    "    linear_model = tf.keras.Sequential([\n",
    "        tf.keras.layers.InputLayer(N),\n",
    "        tf.keras.layers.Dense(1, use_bias=False, kernel_initializer='glorot_normal'),\n",
    "    ])\n",
    "    \n",
    "    train_features = nonlinear_model(X_train)\n",
    "    test_features = nonlinear_model(X_test)\n",
    "    \n",
    "    for lamb in [1e-13, 0]:\n",
    "        svm = SVM(penalty='l2', loss='squared_hinge', fit_intercept=False, dual=False, C=1/max(lamb, 1e-13))\n",
    "        svm = svm.fit(train_features, y_train)\n",
    "\n",
    "        if lamb > 0:\n",
    "            #Initialize NN with SVM weights\n",
    "            linear_model.layers[0].set_weights([svm.coef_.T])\n",
    "\n",
    "        else:\n",
    "            #Initialize NN *near* SVM weights, and we will fine tune with no regularization\n",
    "            linear_model.layers[0].set_weights([svm.coef_.T + np.random.randn(*svm.coef_.T.shape)/np.sqrt(N)])\n",
    "\n",
    "            linear_model.compile(loss='squared_hinge', optimizer='adam')\n",
    "            n_steps = 500\n",
    "            batch_size = min(1024, P//2)\n",
    "            train_data = (\n",
    "                tf.data.Dataset.from_tensor_slices(\n",
    "                    (train_features, y_train.reshape(-1,1).astype(float))\n",
    "                )\n",
    "                .shuffle(buffer_size=min(5*batch_size,P))\n",
    "                .repeat()\n",
    "                .batch(batch_size, drop_remainder=True)\n",
    "            )\n",
    "            for batch in tqdm.tqdm(train_data.take(n_steps), total=n_steps, leave=False):\n",
    "                linear_model.train_step(batch)\n",
    "\n",
    "        p_train = linear_model(train_features).numpy().ravel()\n",
    "        p_test = linear_model(test_features).numpy().ravel()\n",
    "\n",
    "        result = {\n",
    "            'N': N,\n",
    "            'P': P,\n",
    "            'lambda': lamb,\n",
    "            'y_train': y_train.reshape(-1),\n",
    "            'y_train_hat': np.array(p_train).reshape(-1),\n",
    "            'y_test': y_test.reshape(-1),\n",
    "            'y_test_hat': np.array(p_test).reshape(-1)\n",
    "        }\n",
    "\n",
    "        results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_df = pd.DataFrame(results)\n",
    "# result_df.to_pickle('results/mnist_hinge_CK_zero_reg.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.read_pickle('results/mnist_hinge_CK_zero_reg.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "force = lambda y,f: 1 - y*f\n",
    "loss = lambda y,f: np.mean(np.maximum(0, force(y,f))**2)\n",
    "N_del = lambda y,f: np.sum(force(y,f) >= 0)\n",
    "N_correct = lambda y,f: np.sum(y*f > 0)\n",
    "N_incorrect = lambda y,f: np.sum(y*f < 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df['test_loss'] = np.vectorize(loss)(result_df.y_test, result_df.y_test_hat)\n",
    "result_df['train_loss'] = np.vectorize(loss)(result_df.y_train, result_df.y_train_hat)\n",
    "result_df['N_del'] = np.vectorize(N_del)(result_df.y_train, result_df.y_train_hat)\n",
    "\n",
    "result_df['N/P'] = result_df['N']/result_df['P']\n",
    "result_df['P/N'] = result_df['P']/result_df['N']\n",
    "result_df['N_del/P'] = result_df['N_del']/result_df['P']\n",
    "result_df['N_del/N'] = result_df['N_del']/result_df['N']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "cmap = LinearSegmentedColormap.from_list(\n",
    "    'Mei2019', \n",
    "    np.array([\n",
    "        (243, 232, 29),\n",
    "        (245, 173, 47),\n",
    "        (140, 193, 53),\n",
    "        (50,  191, 133),\n",
    "        (23,  167, 198),\n",
    "        (36,  123, 235),\n",
    "        (53,  69,  252),\n",
    "        (52,  27,  203)\n",
    "    ])/255., \n",
    ")\n",
    "\n",
    "gradient = np.linspace(0, 1, 256)\n",
    "gradient = np.vstack((gradient, gradient))\n",
    "fig = plt.figure(figsize=(6,.5))\n",
    "img = plt.imshow(gradient, aspect='auto', cmap=cmap)\n",
    "title = plt.title('Colormap stolen from Mei2019')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "result_df[result_df['lambda'] == 0].plot('N/P', 'test_loss', ax=ax)\n",
    "result_df[result_df['lambda'] != 0].plot('N/P', 'test_loss', ax=ax)\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,7))\n",
    "n_colors = len(result_df['lambda'].unique())\n",
    "\n",
    "#Test\n",
    "for i, (lamb, df) in enumerate(result_df.groupby('lambda')):\n",
    "    df.plot('N/P', 'test_loss', ax=ax)\n",
    "#Train\n",
    "for i, (lamb, df) in enumerate(result_df.groupby('lambda')):\n",
    "    df.plot('N/P', 'train_loss',ax=ax)\n",
    "    \n",
    "ax.legend(ncol=2, title='Test:                           Train:', title_fontsize=11)\n",
    "ax.set_ylabel('Loss')\n",
    "ax.grid(True, ls='--', alpha=.3)\n",
    "ax.set_title('Hinge loss on MNIST with CK features')\n",
    "\n",
    "fig.savefig('mnist_hinge_loss_ck_features_regularized.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,7))\n",
    "n_colors = len(result_df['lambda'].unique())\n",
    "\n",
    "#Test\n",
    "for i, (lamb, df) in enumerate(result_df.groupby('lambda')):\n",
    "    df.plot('N_del/P', 'test_loss', ax=ax)#, kind='scatter')\n",
    "\n",
    "#Train\n",
    "for i, (lamb, df) in enumerate(result_df.groupby('lambda')):\n",
    "    df.plot('N_del/P', 'train_loss', ax=ax, marker='x')#, kind='scatter')\n",
    "\n",
    "ax.legend(ncol=2, title='Test:                           Train:', title_fontsize=11)\n",
    "ax.set_ylabel('Loss')\n",
    "ax.grid(True, ls='--', alpha=.3)\n",
    "ax.set_title('Hinge loss on MNIST with CK features')\n",
    "\n",
    "# fig.savefig('mnist_hinge_loss_ck_features_regularized.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,7))\n",
    "n_colors = len(result_df['lambda'].unique())\n",
    "\n",
    "#Test\n",
    "for i, (lamb, df) in enumerate(result_df.groupby('lambda')):\n",
    "    df.plot('N_del/N', 'test_loss', label='$\\log_{10}(\\lambda)$='+f'{np.log10(lamb):.1f}', ax=ax, color=cmap(float(i)/(n_colors-1)), kind='scatter')\n",
    "\n",
    "#Train\n",
    "for i, (lamb, df) in enumerate(result_df.groupby('lambda')):\n",
    "    df.plot('N_del/N', 'train_loss', label='$\\log_{10}(\\lambda)$='+f'{np.log10(lamb):.1f}', ax=ax, color=cmap(float(i)/(n_colors-1)), marker='x', kind='scatter')\n",
    "\n",
    "ax.legend(ncol=2, title='Test:                           Train:', title_fontsize=11)\n",
    "ax.set_ylabel('Loss')\n",
    "ax.grid(True, ls='--', alpha=.3)\n",
    "ax.set_title('Hinge loss on MNIST with CK features')\n",
    "plt.xscale('log')\n",
    "# fig.savefig('mnist_hinge_loss_ck_features_regularized.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,7))\n",
    "n_colors = len(result_df['lambda'].unique())\n",
    "\n",
    "for i, (lamb, df) in enumerate(result_df.groupby('lambda')):\n",
    "    df.plot('train_loss', 'N_del/N', ax=ax, label='$\\log_{10}(\\lambda)$='+f'{np.log10(lamb):.1f}', color=cmap(float(i)/(n_colors-1)), marker='x', kind='scatter')\n",
    "\n",
    "ax.legend()\n",
    "ax.grid(True, ls='--', alpha=.3)\n",
    "ax.set_title('Hinge loss on MNIST with CK features')\n",
    "plt.yscale('log')\n",
    "# fig.savefig('mnist_hinge_loss_ck_features_regularized.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,7))\n",
    "n_colors = len(result_df['lambda'].unique())\n",
    "\n",
    "for i, (lamb, df) in enumerate(result_df.groupby('lambda')):\n",
    "    df.plot('P/N', 'N_del/N', label='$\\log_{10}(\\lambda)$='+f'{np.log10(lamb):.1f}', ax=ax, color=cmap(float(i)/(n_colors-1)), kind='scatter')\n",
    "\n",
    "ax.legend()\n",
    "ax.grid(True, ls='--', alpha=.3)\n",
    "ax.set_title('Hinge loss on MNIST with CK features')\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "# fig.savefig('mnist_hinge_loss_ck_features_regularized.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,7))\n",
    "n_colors = len(result_df['lambda'].unique())\n",
    "\n",
    "for i, (lamb, df) in enumerate(result_df.groupby('lambda')):\n",
    "    df.plot('P/N', 'N_del/N', label='$\\log_{10}(\\lambda)$='+f'{np.log10(lamb):.1f}', ax=ax, color=cmap(float(i)/(n_colors-1)), kind='scatter')\n",
    "\n",
    "ax.legend()\n",
    "ax.grid(True, ls='--', alpha=.3)\n",
    "ax.set_title('Hinge loss on MNIST with CK features')\n",
    "plt.ylim(.0,5)\n",
    "plt.xlim(0,15)\n",
    "# plt.yscale('log')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:thesis]",
   "language": "python",
   "name": "conda-env-thesis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
