{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext line_profiler\n",
    "\n",
    "import tensorflow as tf\n",
    "import neural_tangents as nt\n",
    "from neural_tangents import stax\n",
    "\n",
    "from jax.config import config; config.update(\"jax_enable_x64\", True)\n",
    "import jax.numpy as np\n",
    "from jax import random, jit\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_h(N, L, d, n=1, bias=True):\n",
    "    # Modified from https://github.com/mariogeiger/nn_jamming/blob/master/constN.py\n",
    "    # TODO: bias=False?\n",
    "    '''\n",
    "        For a network with: \n",
    "        \n",
    "        d input dimensionality, \n",
    "        L layers, \n",
    "        N total parameters, \n",
    "        n final outputs,\n",
    "        \n",
    "        this finds the corresponding width h \n",
    "    '''\n",
    "    assert L >= 1\n",
    "\n",
    "    if L == 1:\n",
    "        # solve : N = h*(d+1) + n*(h+1)\n",
    "        h = (N - n) / (d + n + 1)\n",
    "    else:\n",
    "        # solve : N = h*(d+1) + (L-1)*h*(h+1) + n*(h+1)\n",
    "        h = -(d+L+n - ((d+L+n)**2 + 4*(L-1)*(N-n))**.5)/(2*(L-1))\n",
    "    return round(h)\n",
    "\n",
    "def find_N(h, L, d, n=1):\n",
    "    return h*(d+1) + (L-1)*h*(h+1) + n*(h+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from https://www.openml.org/d/554\n",
    "from sklearn.datasets import fetch_openml\n",
    "X_raw, y_raw = fetch_openml('mnist_784', version=1, return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = 2000 #train\n",
    "P_total = int(1.75*P)\n",
    "\n",
    "X = X_raw[:P_total]\n",
    "y = (2*(y_raw.astype(int) % 2) - 1)[:P_total].reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=1-P/P_total, random_state=42)\n",
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "n_components = 10\n",
    "pca = PCA(n_components = n_components)\n",
    "pca = pca.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pca.transform(X_train)\n",
    "X_test = pca.transform(X_test)\n",
    "\n",
    "# project to hyper-sphere of radius sqrt(n_components)\n",
    "X_train = np.sqrt(n_components) * X_train / np.linalg.norm(X_train, axis = 1, keepdims=True)\n",
    "X_test = np.sqrt(n_components) * X_test / np.linalg.norm(X_test, axis = 1, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = n_components\n",
    "L = 2\n",
    "N = 4*P\n",
    "\n",
    "h = find_h(N, L, d)\n",
    "N/P, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_fn, apply_fn, kernel_fn_inf = stax.serial(\n",
    "    *[stax.Dense(h), stax.Erf()]*L,\n",
    "    stax.Dense(1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initkey = random.PRNGKey(123)\n",
    "_, init_params = init_fn(initkey, X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.api import jacrev\n",
    "from jax import tree_util\n",
    "\n",
    "@jit\n",
    "def jacobian_leaf_flattener(x):\n",
    "    #Flatten everything but first two dimensions (batch_size, n_out)\n",
    "    param_count = int(np.prod(x.shape[2:]))\n",
    "    return np.reshape(x, x.shape[:2] + (param_count,))\n",
    "\n",
    "def network_jacobian(apply_fn, params, inputs):\n",
    "    \"\"\"\n",
    "        TODO: speed me up/find a way to @jit me?\n",
    "    \"\"\"\n",
    "    jac = jacrev(apply_fn)(params, inputs)\n",
    "    leaves, _ = tree_util.tree_flatten(jac)\n",
    "    \n",
    "    return np.concatenate([jacobian_leaf_flattener(leaf) for leaf in leaves], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def mse(y_pred, y_true):\n",
    "    return np.mean(np.sum((y_pred - y_true)**2, -1))\n",
    "\n",
    "@jit\n",
    "def acc(y_pred, y_true):\n",
    "    return np.mean((y_pred*y_true) > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def lstsq_reg(X, y, lamb=0., rcond=None):\n",
    "    \"\"\"\n",
    "        Computes coefficients `b` for the regularized least squares problem\n",
    "            b = argmin_b |(y-Xb)|^2 + lamb*|b|^2\n",
    "        \n",
    "        Based on algorithm for pseudoinverse from \n",
    "            https://github.com/numpy/numpy/blob/v1.17.0/numpy/linalg/linalg.py#L1890-L1979\n",
    "        and\n",
    "            https://jax.readthedocs.io/en/latest/_modules/jax/numpy/linalg.html#lstsq\n",
    "        with modification to regularized pseudoinverse given by\n",
    "            https://en.wikipedia.org/wiki/Tikhonov_regularization#Relation_to_singular-value_decomposition_and_Wiener_filter\n",
    "    \"\"\"\n",
    "    X = np.conj(X)\n",
    "    \n",
    "    u, s, v = np.linalg.svd(X, full_matrices=False)\n",
    "    \n",
    "    if rcond is None:\n",
    "        dtype = X.dtype\n",
    "        rcond = np.finfo(train_features.dtype).eps * max(X.shape)\n",
    "    cutoff = rcond * np.max(s)\n",
    "    mask = s >= cutoff\n",
    "    safe_s = np.where(mask, s, 1.)\n",
    "    \n",
    "    d = np.where(mask, safe_s / (safe_s**2 + lamb**2), 0.)\n",
    "    b = v.T @ (u * d).T @ y \n",
    "    \n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_fn = jit(apply_fn)\n",
    "train_f0 = apply_fn(init_params, X_train)\n",
    "test_f0 = apply_fn(init_params, X_test)\n",
    "\n",
    "train_features = network_jacobian(apply_fn, init_params, X_train).reshape(len(X_train), -1)\n",
    "test_features = network_jacobian(apply_fn, init_params, X_test).reshape(len(X_test), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeffs = lstsq_reg(train_features, y_train-train_f0, lamb = 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = train_f0 + train_features @ coeffs\n",
    "y_test_pred = test_f0 + test_features @ coeffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse(y_test_pred, y_test), acc(y_test_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse(y_train_pred, y_train), acc(y_train_pred, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lamb = 1e-8\n",
    "\n",
    "train_mse = []\n",
    "train_acc = []\n",
    "\n",
    "test_mse = []\n",
    "test_acc = []\n",
    "\n",
    "Ns = []\n",
    "\n",
    "for hi in tqdm.trange(h, 0, -1):\n",
    "    Ns.append(find_N(hi, L, d))\n",
    "    \n",
    "    init_fn, apply_fn, kernel_fn_inf = stax.serial(\n",
    "        *[stax.Dense(hi), stax.Erf()]*L,\n",
    "        stax.Dense(1)\n",
    "    )\n",
    "    apply_fn = jit(apply_fn)\n",
    "    _, init_params = init_fn(initkey, X_train.shape)\n",
    "    \n",
    "    train_f0 = apply_fn(init_params, X_train)\n",
    "    test_f0 = apply_fn(init_params, X_test)\n",
    "\n",
    "    train_features = network_jacobian(apply_fn, init_params, X_train)\n",
    "    train_features = train_features.reshape(len(X_train), -1)\n",
    "    \n",
    "    test_features = network_jacobian(apply_fn, init_params, X_test)\n",
    "    test_features.reshape(len(X_test), -1)\n",
    "    \n",
    "    coeffs = lstsq_reg(train_features, y_train-train_f0, lamb=lamb)\n",
    "    \n",
    "    y_train_pred = train_f0 + train_features @ coeffs\n",
    "    y_test_pred = test_f0 + test_features @ coeffs\n",
    "    \n",
    "    train_mse.append(mse(y_train_pred, y_train))\n",
    "    train_acc.append(acc(y_train_pred, y_train))\n",
    "    \n",
    "    test_mse.append(mse(y_test_pred, y_test))\n",
    "    test_acc.append(acc(y_test_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,7))\n",
    "plt.plot(np.array(Ns)/P, np.log(np.array(test_mse)), label=f'$\\lambda = ${lamb:.0e}')\n",
    "plt.axvline(1, ls=':', color='k')\n",
    "plt.legend()\n",
    "plt.xlabel('N/P')\n",
    "plt.ylabel('log MSE')\n",
    "plt.title('Test MSE on 2-class MNIST.\\nLinear model with fixed NTK features')\n",
    "fig.savefig('test_mse.svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,7))\n",
    "plt.plot(np.array(Ns)/P, np.log(np.array(train_mse)), label=f'$\\lambda = ${lamb:.0e}')\n",
    "plt.axvline(1, ls=':', color='k')\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('N/P')\n",
    "plt.ylabel('log MSE')\n",
    "plt.title('Train MSE on 2-class MNIST.\\nLinear model with fixed NTK features')\n",
    "fig.savefig('train_mse.svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,7))\n",
    "plt.plot(np.array(Ns)/P, test_acc, label=f'$\\lambda = ${lamb:.0e}')\n",
    "plt.axvline(1, ls=':', color='k')\n",
    "plt.legend()\n",
    "plt.xlabel('N/P')\n",
    "plt.ylabel('Acc')\n",
    "plt.title('Test Acc on 2-class MNIST.\\nLinear model with fixed NTK features')\n",
    "fig.savefig('test_acc.svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,7))\n",
    "plt.plot(np.array(Ns)/P, train_acc, label=f'$\\lambda = ${lamb:.0e}')\n",
    "plt.axvline(1, ls=':', color='k')\n",
    "plt.legend()\n",
    "plt.xlabel('N/P')\n",
    "plt.ylabel('Acc')\n",
    "plt.title('Train Acc on 2-class MNIST.\\nLinear model with fixed NTK features')\n",
    "fig.savefig('train_acc.svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:thesis]",
   "language": "python",
   "name": "conda-env-thesis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
