{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post-Processing\n",
    "import pickle\n",
    "import pandas as pd \n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.svm import LinearSVC as SVM\n",
    "\n",
    "import tqdm as tqdm\n",
    "\n",
    "%config InlineBackend.figure_format = 'svg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import colors as mcolors\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "cmap = LinearSegmentedColormap.from_list(\n",
    "    'Mei2019', \n",
    "    np.array([\n",
    "        (243, 232, 29),\n",
    "        (245, 173, 47),\n",
    "        (140, 193, 53),\n",
    "        (50,  191, 133),\n",
    "        (23,  167, 198),\n",
    "        (36,  123, 235),\n",
    "        (53,  69,  252),\n",
    "        (52,  27,  203)\n",
    "    ])/255., \n",
    "    N=256\n",
    ")\n",
    "\n",
    "# cmap = cc.m_bmy\n",
    "\n",
    "gradient = np.linspace(0, 1, 256)\n",
    "gradient = np.vstack((gradient, gradient))\n",
    "fig = plt.figure(figsize=(6,.5))\n",
    "img = plt.imshow(gradient, aspect='auto', cmap=cmap)\n",
    "title = plt.title('Colormap stolen from Mei2019')\n",
    "\n",
    "norm=mcolors.LogNorm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = 'models'\n",
    "model_names = os.listdir(model_dir)\n",
    "X_train, X_test, y_train, y_test = pickle.load(open('mnist.pkl', 'rb'))\n",
    "lambs = np.logspace(-15, -3, num=5)\n",
    "\n",
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in tqdm.tqdm(model_names):\n",
    "    model_path = os.path.join(model_dir, model_name)\n",
    "\n",
    "    P = int(model_name.split('-')[0].split('_')[1])\n",
    "    d = int(model_name.split('-')[1].split('_')[1])\n",
    "    N = int(model_name.split('-')[2].split('_')[1])\n",
    "\n",
    "    full_model = tf.keras.models.load_model(model_path+'/MODEL')\n",
    "    weight_names = [p[:p.find('.index')] for p in os.listdir(model_path) if p.endswith('.ckpt.index')]\n",
    "    weight_names = sorted(weight_names, key = lambda s:int(s.split('_')[1].split('.')[0]))\n",
    "\n",
    "#     for weight_name in tqdm.tqdm(weight_names[-1:], leave=False):\n",
    "    weight_name = weight_names[-1]\n",
    "    for lamb in lambs:\n",
    "        batches_per_epoch = 5\n",
    "        step = int(weight_name.split('.')[0].split('_')[1])*batches_per_epoch\n",
    "\n",
    "        weight_path = os.path.join(model_path, weight_name)\n",
    "        full_model.load_weights(weight_path).expect_partial()\n",
    "\n",
    "        intermed_model = tf.keras.Sequential([\n",
    "            tf.keras.layers.InputLayer(d, name='inputs'),\n",
    "            full_model.get_layer('intermediate')\n",
    "        ])\n",
    "\n",
    "        train_features = intermed_model(X_train)\n",
    "        test_features = intermed_model(X_test)\n",
    "\n",
    "        # Liblinear loss = .5*sum_{i=1}^N[w_i^2] + C*sum_{j=1}^P[y_j - sum_{i=1}^N w_i*x_ji]\n",
    "        # Mei et al loss = (N*lam/d)*sum_{i=1}^N[w_i^2] + (1/P)*sum_{j=1}^P[y_j - sum_{i=1}^N w_i*x_ji]\n",
    "        C = d/(2*N*P*lamb)\n",
    "#             print(C)\n",
    "\n",
    "        svm = SVM(penalty='l2', loss='squared_hinge', dual=False, fit_intercept=False, C=C)\n",
    "        svm = svm.fit(train_features, y_train)\n",
    "\n",
    "        y_train_hat = svm.decision_function(train_features)\n",
    "        y_test_hat = svm.decision_function(test_features)\n",
    "\n",
    "        result = {\n",
    "            \"P\": P,\n",
    "            \"N\": N,\n",
    "            \"d\": d,\n",
    "            \"step\": step,\n",
    "            \"lambda\": lamb,\n",
    "            \"C\": C,\n",
    "            \"y_train_hat\": y_train_hat,\n",
    "            \"y_test_hat\": y_test_hat\n",
    "\n",
    "        }\n",
    "        results.append(result)\n",
    "        \n",
    "# pickle.dump(results, open('results.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "force = lambda y,f: 1 - y*f\n",
    "loss = lambda y,f: np.mean(np.maximum(0, force(y,f))**2, -1)\n",
    "N_del = lambda y,f: np.sum(force(y,f) >= 0, -1)\n",
    "\n",
    "result_df['test_loss'] = result_df.y_test_hat.apply(lambda f: loss(y_test, f))\n",
    "result_df['train_loss'] = result_df.y_train_hat.apply(lambda f: loss(y_train, f))\n",
    "result_df['N_del'] = result_df.y_train_hat.apply(lambda f: N_del(y_train, f))\n",
    "\n",
    "result_df['N/P'] = result_df['N']/result_df['P']\n",
    "result_df['P/N'] = result_df['P']/result_df['N']\n",
    "result_df['N_del/P'] = result_df['N_del']/result_df['P']\n",
    "result_df['N_del/N'] = result_df['N_del']/result_df['N']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = result_df.sort_values('lambda', ascending=False)\n",
    "plt.scatter(data['N_del/N'], data['test_loss'], c=data['lambda'], alpha=.7, cmap=cmap, norm=norm)\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "plt.colorbar(label=r'$\\lambda$ (regularization)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lamb in result_df['lambda'].unique():\n",
    "    data = \n",
    "    plt.scatter(data['N_del/N'], data['test_loss'], c=data['lambda'], alpha=.7, cmap=cmap, norm=norm)\n",
    "    plt.yscale('log')\n",
    "    plt.xscale('log')\n",
    "    plt.colorbar(label=r'$\\lambda$ (regularization)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min(result_df['lambda'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alpha",
   "language": "python",
   "name": "alpha"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
