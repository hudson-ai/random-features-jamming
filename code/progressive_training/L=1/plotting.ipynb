{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = pickle.load(open('mnist.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pickle.load(open('results.pkl', 'rb')) + pickle.load(open('results_end_to_end.pkl', 'rb'))\n",
    "result_df = pd.DataFrame.from_dict(results)\n",
    "\n",
    "force = lambda y,f: 1 - y*f\n",
    "loss = lambda y,f: np.mean(np.maximum(0, force(y,f))**2, -1)\n",
    "N_del = lambda y,f: np.sum(force(y,f) >= 0, -1)\n",
    "\n",
    "result_df['test_loss'] = result_df.y_test_hat.apply(lambda f: loss(y_test, f))\n",
    "result_df['train_loss'] = result_df.y_train_hat.apply(lambda f: loss(y_train, f))\n",
    "result_df['N_del'] = result_df.y_train_hat.apply(lambda f: N_del(y_train, f))\n",
    "\n",
    "result_df['N/P'] = result_df['N']/result_df['P']\n",
    "result_df['P/N'] = result_df['P']/result_df['N']\n",
    "result_df['N_del/P'] = result_df['N_del']/result_df['P']\n",
    "result_df['N_del/N'] = result_df['N_del']/result_df['N']\n",
    "\n",
    "result_end_to_end_df = result_df[result_df['lambda'] == 0]\n",
    "result_df = result_df[result_df['lambda'] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import colors as mcolors\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "cmap = LinearSegmentedColormap.from_list(\n",
    "    'Mei2019', \n",
    "    np.array([\n",
    "        (243, 232, 29),\n",
    "        (245, 173, 47),\n",
    "        (140, 193, 53),\n",
    "        (50,  191, 133),\n",
    "        (23,  167, 198),\n",
    "        (36,  123, 235),\n",
    "        (53,  69,  252),\n",
    "        (52,  27,  203)\n",
    "    ])/255., \n",
    "    N=256\n",
    ")\n",
    "\n",
    "# cmap = cc.m_bmy\n",
    "\n",
    "gradient = np.linspace(0, 1, 256)\n",
    "gradient = np.vstack((gradient, gradient))\n",
    "fig = plt.figure(figsize=(6,.5))\n",
    "img = plt.imshow(gradient, aspect='auto', cmap=cmap)\n",
    "title = plt.title('Colormap stolen from Mei2019')\n",
    "\n",
    "norm=mcolors.LogNorm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,6))\n",
    "data = result_df\n",
    "\n",
    "#invisible plot to set the limits correctly because matplotlib gets confused with log scale scatters\n",
    "x = data['N_del/N']\n",
    "y = data['train_loss']\n",
    "\n",
    "plt.plot(x, y, color='none')\n",
    "plt.scatter(x, y, c=data['step'], cmap=cmap, norm=norm, alpha=.7)\n",
    "plt.colorbar(label='Training steps')\n",
    "\n",
    "plt.xlabel(r'$N_\\Delta/N$')\n",
    "plt.ylabel(r'Train $\\mathcal{L}$')\n",
    "\n",
    "#Plot end_to_end result\n",
    "data = result_end_to_end_df\n",
    "x = data['N_del/N']\n",
    "y = data['train_loss']\n",
    "plt.scatter(x, y, c='k')\n",
    "\n",
    "# plt.yscale('log')\n",
    "# plt.xscale('log')\n",
    "plt.axvline(1, color='k',ls=':')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,6))\n",
    "data = result_df\n",
    "\n",
    "#invisible plot to set the limits correctly because matplotlib gets confused with log scale scatters\n",
    "x = data['N_del/N']\n",
    "y = data['train_loss']\n",
    "\n",
    "plt.plot(x, y, color='none')\n",
    "plt.scatter(x, y, c=data['step'], cmap=cmap, norm=norm, alpha=.7, label=r'Final layer SVM ($\\lambda=1\\times10^{-13}$)')\n",
    "plt.colorbar(label='Training steps')\n",
    "\n",
    "#Plot end_to_end result\n",
    "data = result_end_to_end_df\n",
    "x = data['N_del/N']\n",
    "y = data['train_loss']\n",
    "plt.scatter(x, y, c='k', label='Fully trained network ($\\lambda=0$)', marker='x')\n",
    "\n",
    "plt.xlabel(r'$N_\\Delta/N$')\n",
    "plt.ylabel(r'Train $\\mathcal{L}$')\n",
    "\n",
    "# plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "plt.axvline(1, color='k',ls=':')\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,6))\n",
    "data = result_df\n",
    "\n",
    "#invisible plot to set the limits correctly because matplotlib gets confused with log scale scatters\n",
    "x = data['N_del/N']\n",
    "y = data['train_loss']\n",
    "\n",
    "plt.plot(x, y, color='none')\n",
    "plt.scatter(x, y, c=data['step'], cmap=cmap, norm=norm, alpha=.7, label=r'Final layer SVM ($\\lambda=1\\times10^{-13}$)')\n",
    "plt.colorbar(label='Training steps')\n",
    "\n",
    "#Plot end_to_end result\n",
    "data = result_end_to_end_df\n",
    "x = data['N_del/N']\n",
    "y = data['train_loss']\n",
    "plt.scatter(x, y, c='k', label='Fully trained network ($\\lambda=0$)', marker='x')\n",
    "\n",
    "plt.xlabel(r'$N_\\Delta/N$')\n",
    "plt.ylabel(r'Train $\\mathcal{L}$')\n",
    "\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "plt.axvline(1, color='k',ls=':')\n",
    "plt.axvline(21, color='k',ls=':')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,6))\n",
    "data = result_df.query('step > 1e5')\n",
    "\n",
    "#invisible plot to set the limits correctly because matplotlib gets confused with log scale scatters\n",
    "x = data['N_del/N']\n",
    "y = data['train_loss']\n",
    "\n",
    "plt.plot(x, y, color='none')\n",
    "plt.scatter(x, y, c=data['step'], cmap=cmap, norm=norm, alpha=.7)\n",
    "plt.colorbar(label='Training steps')\n",
    "\n",
    "plt.xlabel(r'$N_\\Delta/N$')\n",
    "plt.ylabel(r'Train $\\mathcal{L}$')\n",
    "\n",
    "# plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "plt.axvline(1, color='k',ls=':')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,6))\n",
    "data = result_df\n",
    "\n",
    "#invisible plot to set the limits correctly because matplotlib gets confused with log scale scatters\n",
    "x = data['P/N']\n",
    "y = data['train_loss']\n",
    "\n",
    "plt.plot(x, y, color='none')\n",
    "plt.scatter(x, y, c=data['step'], cmap=cmap, norm=norm, alpha=.7, label=r'Final layer SVM ($\\lambda=1\\times10^{-13}$)')\n",
    "plt.colorbar(label='Training steps')\n",
    "\n",
    "\n",
    "#Plot end_to_end result\n",
    "data = result_end_to_end_df\n",
    "x = data['P/N']\n",
    "y = data['train_loss']\n",
    "plt.scatter(x, y, c='k', label='Fully trained network ($\\lambda=0$)', marker='x')\n",
    "\n",
    "\n",
    "plt.xlabel(r'$P/N$')\n",
    "plt.ylabel(r'Train $\\mathcal{L}$')\n",
    "\n",
    "# plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,6))\n",
    "data = result_df\n",
    "\n",
    "#invisible plot to set the limits correctly because matplotlib gets confused with log scale scatters\n",
    "x = data['P/N']\n",
    "y = data['train_loss']\n",
    "\n",
    "plt.plot(x, y, color='none')\n",
    "plt.scatter(x, y, c=data['step'], cmap=cmap, norm=norm, alpha=.7, label=r'Final layer SVM ($\\lambda=1\\times10^{-13}$)')\n",
    "plt.colorbar(label='Training steps')\n",
    "\n",
    "\n",
    "#Plot end_to_end result\n",
    "data = result_end_to_end_df\n",
    "x = data['P/N']\n",
    "y = data['train_loss']\n",
    "plt.scatter(x, y, c='k', label='Fully trained network ($\\lambda=0$)', marker='x')\n",
    "\n",
    "\n",
    "plt.xlabel(r'$P/N$')\n",
    "plt.ylabel(r'Train $\\mathcal{L}$')\n",
    "\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,6))\n",
    "data = result_df\n",
    "\n",
    "#invisible plot to set the limits correctly because matplotlib gets confused with log scale scatters\n",
    "x = data['P/N']\n",
    "y = data['test_loss']\n",
    "\n",
    "plt.plot(x, y, color='none')\n",
    "plt.scatter(x, y, c=data['step'], cmap=cmap, norm=norm, alpha=.7, label=r'Final layer SVM ($\\lambda=1\\times10^{-13}$)')\n",
    "plt.colorbar(label='Training steps')\n",
    "\n",
    "#Plot end_to_end result\n",
    "data = result_end_to_end_df\n",
    "x = data['P']/data['N']\n",
    "y = data['test_loss']\n",
    "plt.scatter(x, y, c='k', label='Fully trained network ($\\lambda=0$)', marker='x')\n",
    "\n",
    "plt.xlabel(r'$P/N$')\n",
    "plt.ylabel(r'Test $\\mathcal{L}$')\n",
    "\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,6))\n",
    "\n",
    "for step in sorted(result_df.step.unique())[::15]:\n",
    "    data = result_df.query('step == @step')\n",
    "\n",
    "    #invisible plot to set the limits correctly because matplotlib gets confused with log scale scatters\n",
    "    x = data['P/N']\n",
    "    y = data['test_loss']\n",
    "\n",
    "    plt.plot(x.values[np.argsort(x)], y.values[np.argsort(x)], ls=':', c='k', alpha=.2)\n",
    "    plt.scatter(x, y, c=data['step'], cmap=cmap, norm=norm, alpha=.7)\n",
    "plt.colorbar(label='Training steps')\n",
    "\n",
    "plt.xlabel(r'$P/N$')\n",
    "plt.ylabel(r'Test $\\mathcal{L}$')\n",
    "\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,6))\n",
    "data = result_df\n",
    "\n",
    "#invisible plot to set the limits correctly because matplotlib gets confused with log scale scatters\n",
    "x = data['N_del/N']\n",
    "y = data['test_loss']\n",
    "\n",
    "plt.plot(x, y, color='none')\n",
    "plt.scatter(x, y, c=data['step'], cmap=cmap, norm=norm, alpha=.7, label=r'Final layer SVM ($\\lambda=1\\times10^{-13}$)')\n",
    "plt.colorbar(label='Training steps')\n",
    "\n",
    "#Plot end_to_end result\n",
    "data = result_end_to_end_df\n",
    "x = data['N_del/N']\n",
    "y = data['test_loss']\n",
    "plt.scatter(x, y, c='k', label='Fully trained network ($\\lambda=0$)', marker='x')\n",
    "\n",
    "plt.xlabel(r'$N_\\Delta/N$')\n",
    "plt.ylabel(r'Test $\\mathcal{L}$')\n",
    "\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,6))\n",
    "data = result_df[result_df.step < 1e3]\n",
    "\n",
    "#invisible plot to set the limits correctly because matplotlib gets confused with log scale scatters\n",
    "x = data['N_del/N']\n",
    "y = data['test_loss']\n",
    "\n",
    "plt.plot(x, y, color='none')\n",
    "plt.scatter(x, y, c=data['step'], cmap=cmap, norm=norm, alpha=.7, label=r'Final layer SVM ($\\lambda=1\\times10^{-13}$)')\n",
    "plt.colorbar(label='Training steps')\n",
    "\n",
    "#Plot end_to_end result\n",
    "data = result_end_to_end_df\n",
    "x = data['N_del/N']\n",
    "y = data['test_loss']\n",
    "plt.scatter(x, y, c='k', label='Fully trained network ($\\lambda=0$)', marker='x')\n",
    "\n",
    "plt.xlabel(r'$N_\\Delta/N$')\n",
    "plt.ylabel(r'Test $\\mathcal{L}$')\n",
    "\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,6))\n",
    "data = result_df[result_df.step > 1e3]\n",
    "\n",
    "#invisible plot to set the limits correctly because matplotlib gets confused with log scale scatters\n",
    "x = data['N_del/N']\n",
    "y = data['test_loss']\n",
    "\n",
    "plt.plot(x, y, color='none')\n",
    "plt.scatter(x, y, c=data['step'], cmap=cmap, norm=norm, alpha=.7, label=r'Final layer SVM ($\\lambda=1\\times10^{-13}$)')\n",
    "plt.colorbar(label='Training steps')\n",
    "\n",
    "#Plot end_to_end result\n",
    "data = result_end_to_end_df\n",
    "x = data['N_del/N']\n",
    "y = data['test_loss']\n",
    "plt.scatter(x, y, c='k', label='Fully trained network ($\\lambda=0$)', marker='x')\n",
    "\n",
    "plt.xlabel(r'$N_\\Delta/N$')\n",
    "plt.ylabel(r'Test $\\mathcal{L}$')\n",
    "\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,6))\n",
    "data = result_df[result_df.step > 1e5]\n",
    "\n",
    "#invisible plot to set the limits correctly because matplotlib gets confused with log scale scatters\n",
    "x = data['N_del/N']\n",
    "y = data['test_loss']\n",
    "\n",
    "plt.plot(x, y, color='none')\n",
    "plt.scatter(x, y, c=data['step'], cmap=cmap, norm=norm, alpha=.7, label=r'Final layer SVM ($\\lambda=1\\times10^{-13}$)')\n",
    "plt.colorbar(label='Training steps')\n",
    "\n",
    "#Plot end_to_end result\n",
    "data = result_end_to_end_df\n",
    "x = data['N_del/N']\n",
    "y = data['test_loss']\n",
    "plt.scatter(x, y, c='k', label='Fully trained network ($\\lambda=0$)', marker='x')\n",
    "\n",
    "plt.xlabel(r'$N_\\Delta/N$')\n",
    "plt.ylabel(r'Test $\\mathcal{L}$')\n",
    "\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "\n",
    "plt.legend(loc='lower right')\n",
    "plt.axvline(21, ls=':', c='k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,6))\n",
    "data = result_df\n",
    "\n",
    "#invisible plot to set the limits correctly because matplotlib gets confused with log scale scatters\n",
    "x = data['P/N']\n",
    "y = data['N_del/N']\n",
    "\n",
    "plt.plot(x, y, color='none')\n",
    "plt.scatter(x, y, c=data['step'], cmap=cmap, norm=norm, alpha=.7, label=r'Final layer SVM ($\\lambda=1\\times10^{-13}$)')\n",
    "plt.colorbar(label='Training steps')\n",
    "\n",
    "#Plot end_to_end result\n",
    "data = result_end_to_end_df\n",
    "x = data['P/N']\n",
    "y = data['N_del/N']\n",
    "plt.scatter(x, y, c='k', label='Fully trained network ($\\lambda=0$)', marker='x')\n",
    "\n",
    "plt.xlabel('P/N')\n",
    "plt.ylabel(r'$N_{\\Delta}/N$')\n",
    "\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "plt.axhline(1, color='k',ls=':')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,6))\n",
    "\n",
    "\n",
    "#Plot random features\n",
    "data = result_df[result_df.step == 1]\n",
    "x = data['P/N']\n",
    "y = data['N_del/N']\n",
    "#invisible plot to set the limits correctly because matplotlib gets confused with log scale scatters\n",
    "plt.plot(x, y, color='none')\n",
    "plt.scatter(x, y, c=data['step'], cmap=cmap, norm=norm, alpha=.7, label=r'Final layer SVM ($\\lambda=1\\times10^{-13}$)')\n",
    "plt.colorbar(label='Training steps')\n",
    "\n",
    "\n",
    "#Plot highly trained features\n",
    "data = result_df[result_df.step > 1e5]\n",
    "x = data['P/N']/(data['d']+1)\n",
    "y = data['N_del/N']/(data['d']+1)\n",
    "#invisible plot to set the limits correctly because matplotlib gets confused with log scale scatters\n",
    "plt.plot(x, y, color='none')\n",
    "plt.scatter(x, y, c=data['step'], cmap=cmap, norm=norm, alpha=.7)\n",
    "\n",
    "plt.xlabel(r'$P/ \\tilde N$')\n",
    "plt.ylabel(r'$N_{\\Delta} / \\tilde N$')\n",
    "\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')››\n",
    "plt.axhline(1, color='k',ls=':')\n",
    "\n",
    "# plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,6))\n",
    "\n",
    "\n",
    "#Plot random features\n",
    "data = result_df[result_df.step == 1]\n",
    "x = data['P/N']\n",
    "y = data['N_del/N']\n",
    "#invisible plot to set the limits correctly because matplotlib gets confused with log scale scatters\n",
    "plt.plot(x, y, color='none')\n",
    "plt.scatter(x, y, c=data['step'], cmap=cmap, norm=norm, alpha=.7, label=r'Final layer SVM ($\\lambda=1\\times10^{-13}$)')\n",
    "plt.colorbar(label='Training steps')\n",
    "\n",
    "\n",
    "#Plot highly trained features\n",
    "data = result_df[result_df.step > 1e5]\n",
    "x = data['P/N']/(data['d']+1)\n",
    "y = data['N_del/N']/(data['d']+1)\n",
    "#invisible plot to set the limits correctly because matplotlib gets confused with log scale scatters\n",
    "plt.plot(x, y, color='none')\n",
    "plt.scatter(x, y, c=data['step'], cmap=cmap, norm=norm, alpha=.7)\n",
    "\n",
    "\n",
    "#Plot end_to_end result\n",
    "data = result_end_to_end_df\n",
    "x = data['P']/data['N_tilde']\n",
    "y = data['N_del']/data['N_tilde']\n",
    "#invisible plot to set the limits correctly because matplotlib gets confused with log scale scatters\n",
    "plt.plot(x, y, color='none')\n",
    "plt.scatter(x, y, c='k', label='Fully trained network ($\\lambda=0$)', marker='x')\n",
    "\n",
    "\n",
    "\n",
    "plt.xlabel(r'$P/ \\tilde N$')\n",
    "plt.ylabel(r'$N_{\\Delta} / \\tilde N$')\n",
    "\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "plt.axhline(1, color='k',ls=':')\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why does the threshold $N_\\Delta/N = 1$ persist even throughout training?\n",
    "- maybe it doesn't, but the change in $N_{eff} \\ $ is linear rather than exponential, so it isn't showing up on the log-log plots?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot end_to_end result\n",
    "data = result_end_to_end_df\n",
    "x = data['P']/data['N_tilde']\n",
    "y = data['N_del']/data['N_tilde']\n",
    "plt.scatter(x, y, c='k')\n",
    "# plt.xscale('log')\n",
    "# plt.yscale('log')\n",
    "plt.ylim(0, .1)\n",
    "plt.xlim(1e-3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,6))\n",
    "data = result_df\n",
    "\n",
    "#invisible plot to set the limits correctly because matplotlib gets confused with log scale scatters\n",
    "x = data['N']\n",
    "y = data['N_del']\n",
    "\n",
    "plt.plot(x, y, color='none')\n",
    "plt.scatter(x, y, c=data['step'], cmap=cmap, norm=norm, alpha=.7, label=r'Final layer SVM ($\\lambda=1\\times10^{-13}$)')\n",
    "plt.colorbar(label='Training steps')\n",
    "\n",
    "#Plot end_to_end result\n",
    "data = result_end_to_end_df\n",
    "x = data['N']\n",
    "y = data['N_del']\n",
    "plt.scatter(x, y, c='k', label='Fully trained network ($\\lambda=0$)', marker='x')\n",
    "\n",
    "plt.xlabel('N')\n",
    "plt.ylabel(r'$N_{\\Delta}$')\n",
    "\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:thesis]",
   "language": "python",
   "name": "conda-env-thesis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
