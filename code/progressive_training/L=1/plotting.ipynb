{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import cm\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import colors as mcolors\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "cmap = LinearSegmentedColormap.from_list(\n",
    "    'Mei2019', \n",
    "    np.array([\n",
    "        (243, 232, 29),\n",
    "        (245, 173, 47),\n",
    "        (140, 193, 53),\n",
    "        (50,  191, 133),\n",
    "        (23,  167, 198),\n",
    "        (36,  123, 235),\n",
    "        (53,  69,  252),\n",
    "        (52,  27,  203)\n",
    "    ])/255., \n",
    "    N=256\n",
    ")\n",
    "\n",
    "# cmap = cc.m_bmy\n",
    "\n",
    "gradient = np.linspace(0, 1, 256)\n",
    "gradient = np.vstack((gradient, gradient))\n",
    "fig = plt.figure(figsize=(6,.5))\n",
    "img = plt.imshow(gradient, aspect='auto', cmap=cmap)\n",
    "title = plt.title('Colormap stolen from Mei2019')\n",
    "\n",
    "norm=mcolors.LogNorm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SMALL_SIZE = 14\n",
    "MEDIUM_SIZE = 18\n",
    "BIGGER_SIZE = 20\n",
    "BIGGEST_SIZE = 20\n",
    "\n",
    "plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=BIGGER_SIZE)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize\n",
    "plt.rc('figure', titlesize=BIGGEST_SIZE)  # fontsize of the figure title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance_matrix\n",
    "def smooth(x, y, h=1):\n",
    "    K = np.exp(-distance_matrix(x.values.reshape(-1,1), x.values.reshape(-1,1))**2/(2*h))\n",
    "    return (K@y) / (K@np.ones_like(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = pickle.load(open('mnist.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pickle.load(open('results_with_hessian.pkl', 'rb')) + pickle.load(open('results_end_to_end.pkl', 'rb'))\n",
    "result_df = pd.DataFrame.from_dict(results)\n",
    "\n",
    "\n",
    "result_df['L'] = 1\n",
    "result_df['h'] = result_df['N']\n",
    "result_df['N'] = result_df['h']*(result_df['d'] + 1)\n",
    "\n",
    "force = lambda y,f: 1 - y*f\n",
    "loss = lambda y,f: np.mean(np.maximum(0, force(y,f))**2, -1)\n",
    "N_del = lambda y,f: np.sum(force(y,f) >= 0, -1)\n",
    "\n",
    "result_df['test_loss'] = result_df.y_test_hat.apply(lambda f: loss(y_test, f))\n",
    "result_df['train_loss'] = result_df.y_train_hat.apply(lambda f: loss(y_train, f))\n",
    "result_df['N_del'] = result_df.y_train_hat.apply(lambda f: N_del(y_train, f))\n",
    "\n",
    "result_df['P/N'] = result_df['P']/result_df['N']\n",
    "result_df['N_del/N'] = result_df['N_del']/result_df['N']\n",
    "\n",
    "result_df['P/h'] = result_df['P']/result_df['h']\n",
    "result_df['N_del/h'] = result_df['N_del']/result_df['h']\n",
    "\n",
    "result_end_to_end_df = result_df[result_df['lambda'] == 0]\n",
    "result_df = result_df[result_df['lambda'] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "star_cutoff = 1e-2\n",
    "\n",
    "N_star = result_df.groupby('step').apply(lambda df: df.query('(train_loss > @star_cutoff)')['N'].max())\n",
    "result_df['N_star'] = result_df['step'].map(N_star)\n",
    "\n",
    "h_star = result_df.groupby('step').apply(lambda df: df.query('(train_loss > @star_cutoff)')['h'].max())\n",
    "result_df['h_star'] = result_df['step'].map(h_star)\n",
    "\n",
    "result_df = result_df.query('N_star < 4e4') #cut out outliers\n",
    "\n",
    "\n",
    "N_star = result_df.groupby('step').apply(lambda df: df.query('(train_loss > @star_cutoff)')['N'].max())\n",
    "result_df['N_star'] = result_df['step'].map(N_star)\n",
    "\n",
    "h_star = result_df.groupby('step').apply(lambda df: df.query('(train_loss > @star_cutoff)')['h'].max())\n",
    "result_df['h_star'] = result_df['step'].map(h_star)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(9,6))\n",
    "\n",
    "plt.scatter(h_star.index, h_star)\n",
    "plt.plot(h_star.index, h_star, color='none')\n",
    "\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Training Steps')\n",
    "plt.ylabel(r'$h^*$')\n",
    "plt.ylim(1e1, 1e3)\n",
    "\n",
    "fig.savefig('plots/h_star_vs_train_steps_L=1.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(9,6))\n",
    "\n",
    "by = 'step'\n",
    "x_expr = 'h/P'\n",
    "y_expr = 'train_loss'\n",
    "\n",
    "sm = cm.ScalarMappable(norm=norm, cmap=cmap)\n",
    "\n",
    "by_vals = np.array(sorted(result_df.eval(by).unique(), reverse=False))[::5][::-1] #[[0, 20, 30, 40, 50, 60, 75, 88]]\n",
    "#random zorder helps with visual clarity \n",
    "extremes = by_vals[[0, -1]]\n",
    "by_vals = by_vals[1:-1]\n",
    "by_vals = np.random.choice(by_vals, size=len(by_vals), replace=False)\n",
    "by_vals = np.append(by_vals, extremes)\n",
    "norm.autoscale(by_vals)\n",
    "\n",
    "for val in by_vals: #[::-1]:\n",
    "    color = cmap(norm(val))\n",
    "    \n",
    "    data = result_df.query(f'{by} == @val')\n",
    "    data = data.iloc[np.argsort(data.eval(x_expr))]\n",
    "    \n",
    "    x = data.eval(x_expr)\n",
    "    y = data.eval(y_expr)\n",
    "    plt.scatter(x, y, c=data.eval(by), cmap=cmap, norm=norm, alpha=.7)\n",
    "    \n",
    "    y_sm = smooth(np.log(x), y, .001)\n",
    "    plt.plot(x, y_sm, color=color, ls=':', zorder=-1)\n",
    "\n",
    "plt.colorbar(sm, label='Training Steps')\n",
    "plt.xlabel(x_expr)\n",
    "plt.ylabel(y_expr)\n",
    "\n",
    "# plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "# plt.axhline(star_cutoff, c='k', ls=':')\n",
    "\n",
    "plt.xlabel(r\"$h/P$\")\n",
    "plt.ylabel(r\"Train $\\mathcal{L}$\")\n",
    "# plt.title('L=1')\n",
    "fig.savefig('plots/h_P_vs_train_loss_L=1_linear.pdf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(9,6))\n",
    "\n",
    "by = 'step'\n",
    "x_expr = 'h/P'\n",
    "y_expr = 'train_loss'\n",
    "\n",
    "sm = cm.ScalarMappable(norm=norm, cmap=cmap)\n",
    "\n",
    "by_vals = np.array(sorted(result_df.eval(by).unique(), reverse=False))[::5][::-1] #[[0, 20, 30, 40, 50, 60, 75, 88]]\n",
    "#random zorder helps with visual clarity \n",
    "extremes = by_vals[[0, -1]]\n",
    "by_vals = by_vals[1:-1]\n",
    "by_vals = np.random.choice(by_vals, size=len(by_vals), replace=False)\n",
    "by_vals = np.append(by_vals, extremes)\n",
    "norm.autoscale(by_vals)\n",
    "\n",
    "for val in by_vals: #[::-1]:\n",
    "    color = cmap(norm(val))\n",
    "    \n",
    "    data = result_df.query(f'{by} == @val')\n",
    "    data = data.iloc[np.argsort(data.eval(x_expr))]\n",
    "    \n",
    "    x = data.eval(x_expr)\n",
    "    y = data.eval(y_expr)\n",
    "    plt.scatter(x, y, c=data.eval(by), cmap=cmap, norm=norm, alpha=.7)\n",
    "    \n",
    "    y_sm = smooth(np.log(x), y, .01)\n",
    "    plt.plot(x, y_sm, color=color, ls=':', zorder=-1)\n",
    "\n",
    "plt.colorbar(sm, label='Training Steps')\n",
    "plt.xlabel(x_expr)\n",
    "plt.ylabel(y_expr)\n",
    "\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "plt.axhline(star_cutoff, c='k', ls=':')\n",
    "\n",
    "plt.xlabel(r\"$h/P$\")\n",
    "plt.ylabel(r\"Train $\\mathcal{L}$\")\n",
    "# plt.title('L=1')\n",
    "fig.savefig('plots/h_P_vs_train_loss_L=1_log.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(9,6))\n",
    "\n",
    "by = 'step'\n",
    "x_expr = 'h/h_star'\n",
    "y_expr = 'train_loss'\n",
    "\n",
    "sm = cm.ScalarMappable(norm=norm, cmap=cmap)\n",
    "\n",
    "by_vals = np.array(sorted(result_df.eval(by).unique(), reverse=False))[::5][::-1] #[[0, 20, 30, 40, 50, 60, 75, 88]]\n",
    "#random zorder helps with visual clarity \n",
    "extremes = by_vals[[0, -1]]\n",
    "by_vals = by_vals[1:-1]\n",
    "by_vals = np.random.choice(by_vals, size=len(by_vals), replace=False)\n",
    "by_vals = np.append(by_vals, extremes)\n",
    "norm.autoscale(by_vals)\n",
    "\n",
    "for val in by_vals: #[::-1]:\n",
    "    color = cmap(norm(val))\n",
    "    \n",
    "    data = result_df.query(f'{by} == @val')\n",
    "    data = data.iloc[np.argsort(data.eval(x_expr))]\n",
    "    \n",
    "    x = data.eval(x_expr)\n",
    "    y = data.eval(y_expr)\n",
    "    plt.scatter(x, y, c=data.eval(by), cmap=cmap, norm=norm, alpha=.7)\n",
    "    \n",
    "    y_sm = smooth(np.log(x), y, .01)\n",
    "    plt.plot(x, y_sm, color=color, ls=':', zorder=-1)\n",
    "\n",
    "plt.colorbar(sm, label='Training Steps')\n",
    "plt.xlabel(x_expr)\n",
    "plt.ylabel(y_expr)\n",
    "\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "plt.axvline(1, c='k', ls='--')\n",
    "\n",
    "\n",
    "plt.xlabel(r\"$h/h^*$\")\n",
    "plt.ylabel(r\"Train $\\mathcal{L}$\")\n",
    "fig.savefig('plots/h_h_star_vs_train_loss_L=1_log.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(9,6))\n",
    "\n",
    "by = 'step'\n",
    "x_expr = 'h/P'\n",
    "y_expr = 'test_loss'\n",
    "\n",
    "sm = cm.ScalarMappable(norm=norm, cmap=cmap)\n",
    "\n",
    "by_vals = np.array(sorted(result_df.eval(by).unique(), reverse=False))[::5][::-1] #[[0, 20, 30, 40, 50, 60, 75, 88]]\n",
    "#random zorder helps with visual clarity \n",
    "extremes = by_vals[[0, -1]]\n",
    "by_vals = by_vals[1:-1]\n",
    "by_vals = np.random.choice(by_vals, size=len(by_vals), replace=False)\n",
    "by_vals = np.append(by_vals, extremes)\n",
    "norm.autoscale(by_vals)\n",
    "\n",
    "for val in by_vals: #[::-1]:\n",
    "    color = cmap(norm(val))\n",
    "    \n",
    "    data = result_df.query(f'{by} == @val')\n",
    "    data = data.iloc[np.argsort(data.eval(x_expr))]\n",
    "    \n",
    "    x = data.eval(x_expr)\n",
    "    y = data.eval(y_expr)\n",
    "    plt.scatter(x, y, c=data.eval(by), cmap=cmap, norm=norm, alpha=.7)\n",
    "    \n",
    "    y_sm = smooth(np.log(x), y, .001)\n",
    "    plt.plot(x, y_sm, color=color, ls=':', zorder=-1)\n",
    "\n",
    "plt.colorbar(sm, label='Training Steps')\n",
    "plt.xlabel(x_expr)\n",
    "plt.ylabel(y_expr)\n",
    "\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "# plt.axhline(5e-2, c='k')#, ls=':')\n",
    "\n",
    "plt.xlabel(r\"$h/P$\")\n",
    "plt.ylabel(r\"Test $\\mathcal{L}$\")\n",
    "fig.savefig('plots/h_P_vs_test_loss_L=1_log.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(9,6))\n",
    "\n",
    "by = 'step'\n",
    "x_expr = 'h/h_star'\n",
    "y_expr = 'test_loss'\n",
    "\n",
    "sm = cm.ScalarMappable(norm=norm, cmap=cmap)\n",
    "\n",
    "by_vals = np.array(sorted(result_df.eval(by).unique(), reverse=False))[::5][::-1] #[[0, 20, 30, 40, 50, 60, 75, 88]]\n",
    "#random zorder helps with visual clarity \n",
    "extremes = by_vals[[0, -1]]\n",
    "by_vals = by_vals[1:-1]\n",
    "by_vals = np.random.choice(by_vals, size=len(by_vals), replace=False)\n",
    "by_vals = np.append(by_vals, extremes)\n",
    "norm.autoscale(by_vals)\n",
    "\n",
    "for val in by_vals: #[::-1]:\n",
    "    color = cmap(norm(val))\n",
    "    \n",
    "    data = result_df.query(f'{by} == @val')\n",
    "    data = data.iloc[np.argsort(data.eval(x_expr))]\n",
    "    \n",
    "    x = data.eval(x_expr)\n",
    "    y = data.eval(y_expr)\n",
    "    plt.scatter(x, y, c=data.eval(by), cmap=cmap, norm=norm, alpha=.7)\n",
    "    \n",
    "    y_sm = smooth(np.log(x), y, .001)\n",
    "    plt.plot(x, y_sm, color=color, ls=':', zorder=-1)\n",
    "    \n",
    "plt.colorbar(sm, label='Training Steps')\n",
    "plt.xlabel(x_expr)\n",
    "plt.ylabel(y_expr)\n",
    "\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "# plt.xlim(0, 2)\n",
    "plt.axvline(1, c='k', ls='--')\n",
    "\n",
    "plt.xlabel(r\"$h/h^*$\")\n",
    "plt.ylabel(r\"Test $\\mathcal{L}$\")\n",
    "fig.savefig('plots/h_h_star_vs_test_loss_L=1_log.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(9,6))\n",
    "\n",
    "by = 'step'\n",
    "x_expr = 'h/h_star'\n",
    "y_expr = 'N_del/h'\n",
    "\n",
    "sm = cm.ScalarMappable(norm=norm, cmap=cmap)\n",
    "\n",
    "by_vals = np.array(sorted(result_df.eval(by).unique(), reverse=False))[::5][::-1] #[[0, 20, 30, 40, 50, 60, 75, 88]]\n",
    "#random zorder helps with visual clarity \n",
    "extremes = by_vals[[0, -1]]\n",
    "by_vals = by_vals[1:-1]\n",
    "by_vals = np.random.choice(by_vals, size=len(by_vals), replace=False)\n",
    "by_vals = np.append(by_vals, extremes)\n",
    "norm.autoscale(by_vals)\n",
    "\n",
    "for val in by_vals: #[::-1]:\n",
    "    color = cmap(norm(val))\n",
    "    \n",
    "    data = result_df.query(f'{by} == @val')\n",
    "    data = data.iloc[np.argsort(data.eval(x_expr))]\n",
    "    \n",
    "    x = data.eval(x_expr)\n",
    "    y = data.eval(y_expr)\n",
    "    plt.scatter(x, y, c=data.eval(by), cmap=cmap, norm=norm, alpha=.7)\n",
    "    \n",
    "    y_sm = smooth(np.log(x), y, .001)\n",
    "    plt.plot(x, y_sm, color=color, ls=':')\n",
    "\n",
    "plt.colorbar(sm, label='Training Steps')\n",
    "plt.xlabel(x_expr)\n",
    "plt.ylabel(y_expr)\n",
    "\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "# plt.xlim(0, 2)\n",
    "plt.axvline(1, c='k', ls='--', alpha=.7)\n",
    "plt.axhline(1, c='k', ls='--', alpha=.7)\n",
    "\n",
    "\n",
    "plt.xlabel(r\"$h/h^*$\")\n",
    "plt.ylabel(r\"$N_\\Delta/h$\")\n",
    "fig.savefig('plots/h_h_star_vs_N_del_h_L=1.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(9,6))\n",
    "\n",
    "# by = 'step'\n",
    "x_expr = 'h/h_star'\n",
    "y_expr = 'N_del/N'\n",
    "\n",
    "sm = cm.ScalarMappable(norm=norm, cmap=cmap)\n",
    "\n",
    "# by_vals = np.array(sorted(result_df.eval(by).unique(), reverse=False))[::5][::-1] #[[0, 20, 30, 40, 50, 60, 75, 88]]\n",
    "# norm.autoscale(by_vals)\n",
    "\n",
    "for val in by_vals: #[::-1]:\n",
    "    color = cmap(norm(val))\n",
    "    \n",
    "    data = result_df.query(f'{by} == @val')\n",
    "    data = data.iloc[np.argsort(data.eval(x_expr))]\n",
    "    \n",
    "    x = data.eval(x_expr)\n",
    "    y = data.eval(y_expr)\n",
    "    plt.scatter(x, y, c=data.eval(by), cmap=cmap, norm=norm, alpha=.7)\n",
    "    \n",
    "    y_sm = smooth(np.log(x), y, .001)\n",
    "    plt.plot(x, y_sm, color=color, ls=':')\n",
    "\n",
    "plt.colorbar(sm, label='Training Steps')\n",
    "plt.xlabel(x_expr)\n",
    "plt.ylabel(y_expr)\n",
    "\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "# plt.xlim(0, 2)\n",
    "plt.axvline(1, c='k', ls='--', alpha=.7)\n",
    "plt.axhline(1, c='k', ls='--', alpha=.7)\n",
    "\n",
    "\n",
    "plt.xlabel(r\"$h/h^*$\")\n",
    "plt.ylabel(r\"$N_\\Delta/N$\")\n",
    "fig.savefig('plots/h_h_star_vs_N_del_N_L=1.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(9,6))\n",
    "\n",
    "by = 'step'\n",
    "x_expr = 'train_loss'\n",
    "y_expr = 'N_del/h'\n",
    "\n",
    "sm = cm.ScalarMappable(norm=norm, cmap=cmap)\n",
    "\n",
    "by_vals = np.array(sorted(result_df.eval(by).unique(), reverse=False))[::5][::-1] #[[0, 20, 30, 40, 50, 60, 75, 88]]\n",
    "#random zorder helps with visual clarity \n",
    "extremes = by_vals[[0, -1]]\n",
    "by_vals = by_vals[1:-1]\n",
    "by_vals = np.random.choice(by_vals, size=len(by_vals), replace=False)\n",
    "by_vals = np.append(by_vals, extremes)\n",
    "norm.autoscale(by_vals)\n",
    "\n",
    "for val in by_vals: #[::-1]:\n",
    "    color = cmap(norm(val))\n",
    "    \n",
    "    data = result_df.query(f'{by} == @val')\n",
    "    data = data.iloc[np.argsort(data.eval(x_expr))]\n",
    "    \n",
    "    x = data.eval(x_expr)\n",
    "    y = data.eval(y_expr)\n",
    "    plt.scatter(x, y, c=data.eval(by), cmap=cmap, norm=norm, alpha=.7)\n",
    "    \n",
    "#     y_sm = smooth(np.log(x), y, .001)\n",
    "#     plt.plot(x, y_sm, color=color, ls=':')\n",
    "\n",
    "plt.colorbar(sm, label='Training Steps')\n",
    "plt.xlabel(x_expr)\n",
    "plt.ylabel(y_expr)\n",
    "\n",
    "# plt.yscale('log')\n",
    "# plt.xscale('log')\n",
    "plt.ylim(0, 2)\n",
    "plt.axhline(1, c='k', ls='--', alpha=.7)\n",
    "\n",
    "\n",
    "plt.xlabel(r\"Train $\\mathcal{L}$\")\n",
    "plt.ylabel(r\"$N_\\Delta/h$\")\n",
    "# fig.savefig('plots/h_h_star_vs_N_del_h_L=1.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(9,6))\n",
    "\n",
    "by = 'step'\n",
    "x_expr = 'train_loss'\n",
    "y_expr = 'N_del/N'\n",
    "\n",
    "sm = cm.ScalarMappable(norm=norm, cmap=cmap)\n",
    "\n",
    "by_vals = np.array(sorted(result_df.eval(by).unique(), reverse=False))[::5][::-1] #[[0, 20, 30, 40, 50, 60, 75, 88]]\n",
    "#random zorder helps with visual clarity \n",
    "extremes = by_vals[[0, -1]]\n",
    "by_vals = by_vals[1:-1]\n",
    "by_vals = np.random.choice(by_vals, size=len(by_vals), replace=False)\n",
    "by_vals = np.append(by_vals, extremes)\n",
    "norm.autoscale(by_vals)\n",
    "\n",
    "for val in by_vals: #[::-1]:\n",
    "    color = cmap(norm(val))\n",
    "    \n",
    "    data = result_df.query(f'{by} == @val')\n",
    "    data = data.iloc[np.argsort(data.eval(x_expr))]\n",
    "    \n",
    "    x = data.eval(x_expr)\n",
    "    y = data.eval(y_expr)\n",
    "    plt.scatter(x, y, c=data.eval(by), cmap=cmap, norm=norm, alpha=.7)\n",
    "    \n",
    "#     y_sm = smooth(np.log(x), y, .001)\n",
    "#     plt.plot(x, y_sm, color=color, ls=':')\n",
    "\n",
    "plt.colorbar(sm, label='Training Steps')\n",
    "plt.xlabel(x_expr)\n",
    "plt.ylabel(y_expr)\n",
    "\n",
    "plt.yscale('log')\n",
    "# plt.yscale('symlog', linthreshy=1)\n",
    "# plt.xscale('log')\n",
    "# # plt.xlim(0, 2)\n",
    "plt.axhline(1, c='k', ls='--', alpha=.7)\n",
    "# plt.ylim(0, 10)\n",
    "\n",
    "\n",
    "plt.xlabel(r\"Train $\\mathcal{L}$\")\n",
    "plt.ylabel(r\"$N_\\Delta/N$\")\n",
    "# fig.savefig('plots/h_h_star_vs_N_del_h_L=1.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(9,6))\n",
    "\n",
    "by = 'step'\n",
    "x_expr = 'train_loss'\n",
    "y_expr = 'N_del/h'\n",
    "\n",
    "sm = cm.ScalarMappable(norm=norm, cmap=cmap)\n",
    "\n",
    "by_vals = np.array(sorted(result_df.eval(by).unique(), reverse=False))[::5][::-1] #[[0, 20, 30, 40, 50, 60, 75, 88]]\n",
    "#random zorder helps with visual clarity \n",
    "extremes = by_vals[[0, -1]]\n",
    "by_vals = by_vals[1:-1]\n",
    "by_vals = np.random.choice(by_vals, size=len(by_vals), replace=False)\n",
    "by_vals = np.append(by_vals, extremes)\n",
    "norm.autoscale(by_vals)\n",
    "\n",
    "for val in by_vals: #[::-1]:\n",
    "    color = cmap(norm(val))\n",
    "    \n",
    "    data = result_df.query(f'{by} == @val')\n",
    "    data = data.iloc[np.argsort(data.eval(x_expr))]\n",
    "    \n",
    "    x = data.eval(x_expr)\n",
    "    y = data.eval(y_expr)\n",
    "    plt.scatter(x, y, c=data.eval(by), cmap=cmap, norm=norm, alpha=.7)\n",
    "    \n",
    "#     y_sm = smooth(np.log(x), y, .001)\n",
    "#     plt.plot(x, y_sm, color=color, ls=':')\n",
    "\n",
    "plt.colorbar(sm, label='Training Steps')\n",
    "plt.xlabel(x_expr)\n",
    "plt.ylabel(y_expr)\n",
    "\n",
    "# plt.yscale('log')\n",
    "# plt.xscale('log')\n",
    "# # plt.xlim(0, 2)\n",
    "plt.axhline(1, c='k', ls='--', alpha=.7)\n",
    "plt.ylim(0, 10)\n",
    "\n",
    "\n",
    "plt.xlabel(r\"Train $\\mathcal{L}$\")\n",
    "plt.ylabel(r\"$N_\\Delta/N$\")\n",
    "# fig.savefig('plots/h_h_star_vs_N_del_h_L=1.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(9,6))\n",
    "\n",
    "# by = 'step'\n",
    "x_expr = 'N_del/h'\n",
    "y_expr = 'test_loss'\n",
    "\n",
    "sm = cm.ScalarMappable(norm=norm, cmap=cmap)\n",
    "\n",
    "# by_vals = np.array(sorted(result_df.eval(by).unique(), reverse=False))[::5][::-1] #[[0, 20, 30, 40, 50, 60, 75, 88]]\n",
    "# norm.autoscale(by_vals)\n",
    "\n",
    "for val in by_vals: #[::-1]:\n",
    "    color = cmap(norm(val))\n",
    "    \n",
    "    data = result_df.query(f'{by} == @val')\n",
    "    data = data.iloc[np.argsort(data.eval(x_expr))]\n",
    "    \n",
    "    x = data.eval(x_expr)\n",
    "    y = data.eval(y_expr)\n",
    "    plt.scatter(x, y, c=data.eval(by), cmap=cmap, norm=norm, alpha=.7)\n",
    "    \n",
    "    y_sm = y #smooth(np.log(x), y, .0001)\n",
    "    plt.plot(x, y_sm, color=color, ls=':')\n",
    "\n",
    "plt.colorbar(sm, label='Training Steps')\n",
    "plt.xlabel(x_expr)\n",
    "plt.ylabel(y_expr)\n",
    "\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "# plt.xlim(0, 2)\n",
    "plt.axvline(1, c='k', ls='--', alpha=.7)\n",
    "# plt.axhline(1, c='k', ls='--', alpha=.7)\n",
    "\n",
    "\n",
    "# plt.xlabel(r\"$h/h^*$\")\n",
    "# plt.ylabel(r\"$N_\\Delta/N$\")\n",
    "# fig.savefig('plots/h_h_star_vs_N_del_N_L=1.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(9,6))\n",
    "\n",
    "# by = 'step'\n",
    "x_expr = 'N_del/N'\n",
    "y_expr = 'test_loss'\n",
    "\n",
    "sm = cm.ScalarMappable(norm=norm, cmap=cmap)\n",
    "\n",
    "# by_vals = np.array(sorted(result_df.eval(by).unique(), reverse=False))[::5][::-1] #[[0, 20, 30, 40, 50, 60, 75, 88]]\n",
    "# norm.autoscale(by_vals)\n",
    "\n",
    "for val in by_vals: #[::-1]:\n",
    "    color = cmap(norm(val))\n",
    "    \n",
    "    data = result_df.query(f'{by} == @val')\n",
    "    data = data.iloc[np.argsort(data.eval(x_expr))]\n",
    "    \n",
    "    x = data.eval(x_expr)\n",
    "    y = data.eval(y_expr)\n",
    "    plt.scatter(x, y, c=data.eval(by), cmap=cmap, norm=norm, alpha=.7)\n",
    "    \n",
    "    y_sm = y #smooth(np.log(x), y, .0001)\n",
    "    plt.plot(x, y_sm, color=color, ls=':')\n",
    "\n",
    "plt.colorbar(sm, label='Training Steps')\n",
    "plt.xlabel(x_expr)\n",
    "plt.ylabel(y_expr)\n",
    "\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "# plt.xlim(0, 2)\n",
    "plt.axvline(1, c='k', ls='--', alpha=.7)\n",
    "# plt.axhline(1, c='k', ls='--', alpha=.7)\n",
    "\n",
    "\n",
    "# plt.xlabel(r\"$h/h^*$\")\n",
    "# plt.ylabel(r\"$N_\\Delta/N$\")\n",
    "# fig.savefig('plots/h_h_star_vs_N_del_N_L=1.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.set_index('N')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,6))\n",
    "\n",
    "by = 'step'\n",
    "x_expr = 'N'\n",
    "y_expr = 'train_loss'\n",
    "\n",
    "sm = cm.ScalarMappable(norm=norm, cmap=cmap)\n",
    "\n",
    "by_vals = np.array(sorted(result_df.eval(by).unique(), reverse=False))[::5] #[[0, 20, 30, 40, 50, 60, 75, 88]]\n",
    "norm.autoscale(by_vals)\n",
    "\n",
    "for val in by_vals: #[::-1]:\n",
    "    color = cmap(norm(val))\n",
    "    \n",
    "    data = result_df.query(f'{by} == @val')\n",
    "    data = data.iloc[np.argsort(data.eval(x_expr))]\n",
    "    N_star = data.query('train_loss > 3.5e-2')['N'].max()\n",
    "    \n",
    "    x = data.eval(x_expr)/N_star\n",
    "    y = data.eval(y_expr)\n",
    "    plt.scatter(x, y, c=data.eval(by), cmap=cmap, norm=norm, alpha=.7)\n",
    "    \n",
    "    y_sm = smooth(np.log10(x), y, .01)\n",
    "    plt.plot(x, y_sm, color=color, ls=':')\n",
    "\n",
    "plt.colorbar(sm, label=by)\n",
    "plt.xlabel(x_expr)\n",
    "plt.ylabel(y_expr)\n",
    "\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "# plt.ylim(0, 1)\n",
    "# plt.axvline(1, c='k')#, ls=':')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,6))\n",
    "\n",
    "by = 'step'\n",
    "x_expr = 'N_del/h'\n",
    "y_expr = 'test_loss'\n",
    "\n",
    "sm = cm.ScalarMappable(norm=norm, cmap=cmap)\n",
    "\n",
    "by_vals = np.array(sorted(result_df.eval(by).unique(), reverse=False))[::10] #[[0, 20, 30, 40, 50, 60, 75, 88]]\n",
    "norm.autoscale(by_vals)\n",
    "\n",
    "for val in by_vals:\n",
    "    color = cmap(norm(val))\n",
    "    \n",
    "    data = result_df.query(f'{by} == @val')\n",
    "    data = data.iloc[np.argsort(data.eval(x_expr))][::2]\n",
    "    \n",
    "    x = data.eval(x_expr)\n",
    "    y = data.eval(y_expr)\n",
    "    plt.scatter(x, y, c=data.eval(by), cmap=cmap, norm=norm, alpha=.7)\n",
    "    \n",
    "    y_sm = smooth(np.log(x), y, .1)\n",
    "    plt.plot(x, y_sm, color=color, ls=':')\n",
    "\n",
    "plt.colorbar(sm, label=by)\n",
    "plt.xlabel(x_expr)\n",
    "plt.ylabel(y_expr)\n",
    "\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "# plt.axvline(1, c='k')#, ls=':')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,6))\n",
    "data = result_df\n",
    "\n",
    "#invisible plot to set the limits correctly because matplotlib gets confused with log scale scatters\n",
    "x = data['N_del/N']\n",
    "y = data['train_loss']\n",
    "\n",
    "plt.plot(x, y, color='none')\n",
    "plt.scatter(x, y, c=data['step'], cmap=cmap, norm=norm, alpha=.7)\n",
    "plt.colorbar(label='Training steps')\n",
    "\n",
    "plt.xlabel(r'$N_\\Delta/N$')\n",
    "plt.ylabel(r'Train $\\mathcal{L}$')\n",
    "\n",
    "\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "\n",
    "plt.axvline(1, color='k',ls=':')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,6))\n",
    "data = result_df\n",
    "\n",
    "#invisible plot to set the limits correctly because matplotlib gets confused with log scale scatters\n",
    "x = data['N_del/N']\n",
    "y = data['train_loss']\n",
    "\n",
    "plt.plot(x, y, color='none')\n",
    "plt.scatter(x, y, c=data['step'], cmap=cmap, norm=norm, alpha=.7)\n",
    "plt.colorbar(label='Training steps')\n",
    "\n",
    "plt.xlabel(r'$N_\\Delta/N$')\n",
    "plt.ylabel(r'Train $\\mathcal{L}$')\n",
    "\n",
    "#Plot end_to_end result\n",
    "data = result_end_to_end_df\n",
    "x = data['N_del/N']\n",
    "y = data['train_loss']\n",
    "plt.scatter(x, y, c='k')\n",
    "\n",
    "# plt.yscale('log')\n",
    "# plt.xscale('log')\n",
    "plt.axvline(1, color='k',ls=':')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,6))\n",
    "data = result_df\n",
    "\n",
    "#invisible plot to set the limits correctly because matplotlib gets confused with log scale scatters\n",
    "x = data['N_del/h']\n",
    "y = data['train_loss']\n",
    "\n",
    "plt.plot(x, y, color='none')\n",
    "plt.scatter(x, y, c=data['step'], cmap=cmap, norm=norm, alpha=.7, label=r'Final layer SVM ($\\lambda=1\\times10^{-13}$)')\n",
    "plt.colorbar(label='Training steps')\n",
    "\n",
    "#Plot end_to_end result\n",
    "data = result_end_to_end_df\n",
    "x = data['N_del/h']\n",
    "y = data['train_loss']\n",
    "plt.scatter(x, y, c='k', label='Fully trained network ($\\lambda=0$)', marker='x')\n",
    "\n",
    "plt.xlabel(r'$N_\\Delta/h$')\n",
    "plt.ylabel(r'Train $\\mathcal{L}$')\n",
    "\n",
    "# plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "plt.axvline(1, color='k',ls=':')\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,6))\n",
    "data = result_df\n",
    "\n",
    "#invisible plot to set the limits correctly because matplotlib gets confused with log scale scatters\n",
    "x = data['N_del/h']\n",
    "y = data['train_loss']\n",
    "\n",
    "plt.plot(x, y, color='none')\n",
    "plt.scatter(x, y, c=data['step'], cmap=cmap, norm=norm, alpha=.7, label=r'Final layer SVM ($\\lambda=1\\times10^{-13}$)')\n",
    "plt.colorbar(label='Training steps')\n",
    "\n",
    "#Plot end_to_end result\n",
    "data = result_end_to_end_df.query('N < 300')\n",
    "x = data['N_del/h']\n",
    "y = data['train_loss']\n",
    "plt.scatter(x, y, c='k', label='Fully trained network ($\\lambda=0$)', marker='x')\n",
    "\n",
    "plt.xlabel(r'$N_\\Delta/h$')\n",
    "plt.ylabel(r'Train $\\mathcal{L}$')\n",
    "\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "plt.axvline(1, color='k',ls=':')\n",
    "plt.axvline(21, color='k',ls=':')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,6))\n",
    "data = result_df.query('step > 1e5')\n",
    "\n",
    "#invisible plot to set the limits correctly because matplotlib gets confused with log scale scatters\n",
    "x = data['N_del/N']\n",
    "y = data['train_loss']\n",
    "\n",
    "plt.plot(x, y, color='none')\n",
    "plt.scatter(x, y, c=data['step'], cmap=cmap, norm=norm, alpha=.7)\n",
    "plt.colorbar(label='Training steps')\n",
    "\n",
    "plt.xlabel(r'$N_\\Delta/N$')\n",
    "plt.ylabel(r'Train $\\mathcal{L}$')\n",
    "\n",
    "# plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "plt.axvline(1, color='k',ls=':')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,6))\n",
    "data = result_df\n",
    "\n",
    "#invisible plot to set the limits correctly because matplotlib gets confused with log scale scatters\n",
    "x = data['P/N']\n",
    "y = data['train_loss']\n",
    "\n",
    "plt.plot(x, y, color='none')\n",
    "plt.scatter(x, y, c=data['step'], cmap=cmap, norm=norm, alpha=.7, label=r'Final layer SVM ($\\lambda=1\\times10^{-13}$)')\n",
    "plt.colorbar(label='Training steps')\n",
    "\n",
    "\n",
    "#Plot end_to_end result\n",
    "data = result_end_to_end_df\n",
    "x = data['P/N']\n",
    "y = data['train_loss']\n",
    "plt.scatter(x, y, c='k', label='Fully trained network ($\\lambda=0$)', marker='x')\n",
    "\n",
    "\n",
    "plt.xlabel(r'$P/N$')\n",
    "plt.ylabel(r'Train $\\mathcal{L}$')\n",
    "\n",
    "# plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,6))\n",
    "data = result_df\n",
    "\n",
    "#invisible plot to set the limits correctly because matplotlib gets confused with log scale scatters\n",
    "x = data['P/N']\n",
    "y = data['train_loss']\n",
    "\n",
    "plt.plot(x, y, color='none')\n",
    "plt.scatter(x, y, c=data['step'], cmap=cmap, norm=norm, alpha=.7, label=r'Final layer SVM ($\\lambda=1\\times10^{-13}$)')\n",
    "plt.colorbar(label='Training steps')\n",
    "\n",
    "\n",
    "#Plot end_to_end result\n",
    "data = result_end_to_end_df\n",
    "x = data['P/N']\n",
    "y = data['train_loss']\n",
    "plt.scatter(x, y, c='k', label='Fully trained network ($\\lambda=0$)', marker='x')\n",
    "\n",
    "\n",
    "plt.xlabel(r'$P/N$')\n",
    "plt.ylabel(r'Train $\\mathcal{L}$')\n",
    "\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,6))\n",
    "data = result_df\n",
    "\n",
    "#invisible plot to set the limits correctly because matplotlib gets confused with log scale scatters\n",
    "x = data['N']\n",
    "y = data['train_loss']\n",
    "\n",
    "plt.plot(x, y, color='none')\n",
    "plt.scatter(x, y, c=data['step'], cmap=cmap, norm=norm, alpha=.7, label=r'Final layer SVM ($\\lambda=1\\times10^{-13}$)')\n",
    "plt.colorbar(label='Training steps')\n",
    "\n",
    "\n",
    "#Plot end_to_end result\n",
    "data = result_end_to_end_df\n",
    "x = data['N']\n",
    "y = data['train_loss']\n",
    "plt.scatter(x, y, c='k', label='Fully trained network ($\\lambda=0$)', marker='x')\n",
    "\n",
    "\n",
    "plt.xlabel(r'$N$')\n",
    "plt.ylabel(r'Train $\\mathcal{L}$')\n",
    "\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "plt.legend()\n",
    "\n",
    "#invisible plot to set the limits correctly because matplotlib gets confused with log scale scatters\n",
    "x = data['N']\n",
    "y = data['test_loss']\n",
    "\n",
    "plt.plot(x, y, color='none')\n",
    "plt.scatter(x, y, c=data['step'], cmap=cmap, norm=norm, alpha=.7, label=r'Final layer SVM ($\\lambda=1\\times10^{-13}$)')\n",
    "plt.colorbar(label='Training steps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,6))\n",
    "data = result_df\n",
    "\n",
    "#invisible plot to set the limits correctly because matplotlib gets confused with log scale scatters\n",
    "x = data['N']\n",
    "y = data['test_loss']\n",
    "\n",
    "plt.plot(x, y, color='none')\n",
    "plt.scatter(x, y, c=data['step'], cmap=cmap, norm=norm, alpha=.7, label=r'Final layer SVM ($\\lambda=1\\times10^{-13}$)')\n",
    "plt.colorbar(label='Training steps')\n",
    "\n",
    "\n",
    "#Plot end_to_end result\n",
    "data = result_end_to_end_df\n",
    "x = data['N']\n",
    "y = data['test_loss']\n",
    "plt.scatter(x, y, c='k', label='Fully trained network ($\\lambda=0$)', marker='x')\n",
    "\n",
    "\n",
    "plt.xlabel(r'$N$')\n",
    "plt.ylabel(r'Test $\\mathcal{L}$')\n",
    "\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,6))\n",
    "data = result_df\n",
    "\n",
    "#invisible plot to set the limits correctly because matplotlib gets confused with log scale scatters\n",
    "x = data['P/N']\n",
    "y = data['test_loss']\n",
    "\n",
    "plt.plot(x, y, color='none')\n",
    "plt.scatter(x, y, c=data['step'], cmap=cmap, norm=norm, alpha=.7, label=r'Final layer SVM ($\\lambda=1\\times10^{-13}$)')\n",
    "plt.colorbar(label='Training steps')\n",
    "\n",
    "#Plot end_to_end result\n",
    "data = result_end_to_end_df\n",
    "x = data['P']/data['N']\n",
    "y = data['test_loss']\n",
    "plt.scatter(x, y, c='k', label='Fully trained network ($\\lambda=0$)', marker='x')\n",
    "\n",
    "plt.xlabel(r'$P/N$')\n",
    "plt.ylabel(r'Test $\\mathcal{L}$')\n",
    "\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,6))\n",
    "\n",
    "for step in sorted(result_df.step.unique())[::15]:\n",
    "    data = result_df.query('step == @step')\n",
    "\n",
    "    #invisible plot to set the limits correctly because matplotlib gets confused with log scale scatters\n",
    "    x = data['P/N']\n",
    "    y = data['test_loss']\n",
    "\n",
    "    plt.plot(x.values[np.argsort(x)], y.values[np.argsort(x)], ls=':', c='k', alpha=.2)\n",
    "    plt.scatter(x, y, c=data['step'], cmap=cmap, norm=norm, alpha=.7)\n",
    "plt.colorbar(label='Training steps')\n",
    "\n",
    "plt.xlabel(r'$P/N$')\n",
    "plt.ylabel(r'Test $\\mathcal{L}$')\n",
    "\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,6))\n",
    "data = result_df\n",
    "\n",
    "#invisible plot to set the limits correctly because matplotlib gets confused with log scale scatters\n",
    "x = data['N_del/N']\n",
    "y = data['test_loss']\n",
    "\n",
    "plt.plot(x, y, color='none')\n",
    "plt.scatter(x, y, c=data['step'], cmap=cmap, norm=norm, alpha=.7, label=r'Final layer SVM ($\\lambda=1\\times10^{-13}$)')\n",
    "plt.colorbar(label='Training steps')\n",
    "\n",
    "#Plot end_to_end result\n",
    "data = result_end_to_end_df\n",
    "x = data['N_del/N']\n",
    "y = data['test_loss']\n",
    "plt.scatter(x, y, c='k', label='Fully trained network ($\\lambda=0$)', marker='x')\n",
    "\n",
    "plt.xlabel(r'$N_\\Delta/N$')\n",
    "plt.ylabel(r'Test $\\mathcal{L}$')\n",
    "\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,6))\n",
    "data = result_df[result_df.step < 1e3]\n",
    "\n",
    "#invisible plot to set the limits correctly because matplotlib gets confused with log scale scatters\n",
    "x = data['N_del/N']\n",
    "y = data['test_loss']\n",
    "\n",
    "plt.plot(x, y, color='none')\n",
    "plt.scatter(x, y, c=data['step'], cmap=cmap, norm=norm, alpha=.7, label=r'Final layer SVM ($\\lambda=1\\times10^{-13}$)')\n",
    "plt.colorbar(label='Training steps')\n",
    "\n",
    "#Plot end_to_end result\n",
    "data = result_end_to_end_df\n",
    "x = data['N_del/N']\n",
    "y = data['test_loss']\n",
    "plt.scatter(x, y, c='k', label='Fully trained network ($\\lambda=0$)', marker='x')\n",
    "\n",
    "plt.xlabel(r'$N_\\Delta/N$')\n",
    "plt.ylabel(r'Test $\\mathcal{L}$')\n",
    "\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,6))\n",
    "data = result_df[result_df.step > 1e3]\n",
    "\n",
    "#invisible plot to set the limits correctly because matplotlib gets confused with log scale scatters\n",
    "x = data['N_del/N']\n",
    "y = data['test_loss']\n",
    "\n",
    "plt.plot(x, y, color='none')\n",
    "plt.scatter(x, y, c=data['step'], cmap=cmap, norm=norm, alpha=.7, label=r'Final layer SVM ($\\lambda=1\\times10^{-13}$)')\n",
    "plt.colorbar(label='Training steps')\n",
    "\n",
    "#Plot end_to_end result\n",
    "data = result_end_to_end_df\n",
    "x = data['N_del/N']\n",
    "y = data['test_loss']\n",
    "plt.scatter(x, y, c='k', label='Fully trained network ($\\lambda=0$)', marker='x')\n",
    "\n",
    "plt.xlabel(r'$N_\\Delta/N$')\n",
    "plt.ylabel(r'Test $\\mathcal{L}$')\n",
    "\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,6))\n",
    "data = result_df[result_df.step > 1e5]\n",
    "\n",
    "#invisible plot to set the limits correctly because matplotlib gets confused with log scale scatters\n",
    "x = data['N_del/N']\n",
    "y = data['test_loss']\n",
    "\n",
    "plt.plot(x, y, color='none')\n",
    "plt.scatter(x, y, c=data['step'], cmap=cmap, norm=norm, alpha=.7, label=r'Final layer SVM ($\\lambda=1\\times10^{-13}$)')\n",
    "plt.colorbar(label='Training steps')\n",
    "\n",
    "#Plot end_to_end result\n",
    "data = result_end_to_end_df\n",
    "x = data['N_del/N']\n",
    "y = data['test_loss']\n",
    "plt.scatter(x, y, c='k', label='Fully trained network ($\\lambda=0$)', marker='x')\n",
    "\n",
    "plt.xlabel(r'$N_\\Delta/N$')\n",
    "plt.ylabel(r'Test $\\mathcal{L}$')\n",
    "\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "\n",
    "plt.legend(loc='lower right')\n",
    "plt.axvline(21, ls=':', c='k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,6))\n",
    "data = result_df\n",
    "\n",
    "#invisible plot to set the limits correctly because matplotlib gets confused with log scale scatters\n",
    "x = data['P/N']\n",
    "y = data['N_del/N']\n",
    "\n",
    "plt.plot(x, y, color='none')\n",
    "plt.scatter(x, y, c=data['step'], cmap=cmap, norm=norm, alpha=.7, label=r'Final layer SVM ($\\lambda=1\\times10^{-13}$)')\n",
    "plt.colorbar(label='Training steps')\n",
    "\n",
    "#Plot end_to_end result\n",
    "data = result_end_to_end_df\n",
    "x = data['P/N']\n",
    "y = data['N_del/N']\n",
    "plt.scatter(x, y, c='k', label='Fully trained network ($\\lambda=0$)', marker='x')\n",
    "\n",
    "plt.xlabel('P/N')\n",
    "plt.ylabel(r'$N_{\\Delta}/N$')\n",
    "\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "plt.axhline(1, color='k',ls=':')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,6))\n",
    "data = result_df\n",
    "\n",
    "#invisible plot to set the limits correctly because matplotlib gets confused with log scale scatters\n",
    "x = data['N']/data['P']\n",
    "y = data['N_del/N']\n",
    "\n",
    "plt.plot(x, y, color='none')\n",
    "plt.scatter(x, y, c=data['step'], cmap=cmap, norm=norm, alpha=.7, label=r'Final layer SVM ($\\lambda=1\\times10^{-13}$)')\n",
    "plt.colorbar(label='Training steps')\n",
    "\n",
    "#Plot end_to_end result\n",
    "data = result_end_to_end_df\n",
    "x = data['N']/data['P']\n",
    "y = data['N_del/N']\n",
    "plt.scatter(x, y, c='k', label='Fully trained network ($\\lambda=0$)', marker='x')\n",
    "\n",
    "plt.xlabel('N/P')\n",
    "plt.ylabel(r'$N_{\\Delta}/N$')\n",
    "\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "plt.axhline(1, color='k',ls=':')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,6))\n",
    "data = result_df\n",
    "\n",
    "#invisible plot to set the limits correctly because matplotlib gets confused with log scale scatters\n",
    "x = data['h']/data['P']\n",
    "y = data['N_del/h']\n",
    "\n",
    "plt.plot(x, y, color='none')\n",
    "plt.scatter(x, y, c=data['step'], cmap=cmap, norm=norm, alpha=.7, label=r'Final layer SVM ($\\lambda=1\\times10^{-13}$)')\n",
    "plt.colorbar(label='Training steps')\n",
    "\n",
    "#Plot end_to_end result\n",
    "data = result_end_to_end_df\n",
    "x = data['h']/data['P']\n",
    "y = data['N_del/h']\n",
    "plt.scatter(x, y, c='k', label='Fully trained network ($\\lambda=0$)', marker='x')\n",
    "\n",
    "plt.xlabel('h/P')\n",
    "plt.ylabel(r'$N_{\\Delta}/h$')\n",
    "\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "plt.axhline(1, color='k',ls=':')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,6))\n",
    "data = result_df\n",
    "\n",
    "#invisible plot to set the limits correctly because matplotlib gets confused with log scale scatters\n",
    "x = data['N']/data['P']\n",
    "y = data['train_loss']\n",
    "\n",
    "plt.plot(x, y, color='none')\n",
    "plt.scatter(x, y, c=data['step'], cmap=cmap, norm=norm, alpha=.7, label=r'Final layer SVM ($\\lambda=1\\times10^{-13}$)')\n",
    "plt.colorbar(label='Training steps')\n",
    "\n",
    "#Plot end_to_end result\n",
    "data = result_end_to_end_df\n",
    "x = data['N']/data['P']\n",
    "y = data['train_loss']\n",
    "plt.scatter(x, y, c='k', label='Fully trained network ($\\lambda=0$)', marker='x')\n",
    "\n",
    "plt.xlabel('N/P')\n",
    "plt.ylabel(r'Train $\\mathcal{L}$')\n",
    "\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,6))\n",
    "data = result_df\n",
    "\n",
    "#invisible plot to set the limits correctly because matplotlib gets confused with log scale scatters\n",
    "x = data['h']/data['P']\n",
    "y = data['train_loss']\n",
    "\n",
    "plt.plot(x, y, color='none')\n",
    "plt.scatter(x, y, c=data['step'], cmap=cmap, norm=norm, alpha=.7, label=r'Final layer SVM ($\\lambda=1\\times10^{-13}$)')\n",
    "plt.colorbar(label='Training steps')\n",
    "\n",
    "#Plot end_to_end result\n",
    "data = result_end_to_end_df\n",
    "x = data['h']/data['P']\n",
    "y = data['train_loss']\n",
    "plt.scatter(x, y, c='k', label='Fully trained network ($\\lambda=0$)', marker='x')\n",
    "\n",
    "plt.xlabel('h/P')\n",
    "plt.ylabel(r'Train $\\mathcal{L}$')\n",
    "\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,6))\n",
    "data = result_df\n",
    "\n",
    "#invisible plot to set the limits correctly because matplotlib gets confused with log scale scatters\n",
    "x = data['N']/data['P']\n",
    "y = data['test_loss']\n",
    "\n",
    "plt.plot(x, y, color='none')\n",
    "plt.scatter(x, y, c=data['step'], cmap=cmap, norm=norm, alpha=.7, label=r'Final layer SVM ($\\lambda=1\\times10^{-13}$)')\n",
    "plt.colorbar(label='Training steps')\n",
    "\n",
    "#Plot end_to_end result\n",
    "data = result_end_to_end_df\n",
    "x = data['N']/data['P']\n",
    "y = data['test_loss']\n",
    "plt.scatter(x, y, c='k', label='Fully trained network ($\\lambda=0$)', marker='x')\n",
    "\n",
    "plt.xlabel('N/P')\n",
    "plt.ylabel(r'Test $\\mathcal{L}$')\n",
    "\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "plt.legend()\n",
    "# plt.ylim(0, 1)\n",
    "plt.axhline(np.min(data['test_loss']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,6))\n",
    "data = result_df\n",
    "\n",
    "#invisible plot to set the limits correctly because matplotlib gets confused with log scale scatters\n",
    "x = data['h']/data['P']\n",
    "y = data['test_loss']\n",
    "\n",
    "plt.plot(x, y, color='none')\n",
    "plt.scatter(x, y, c=data['step'], cmap=cmap, norm=norm, alpha=.7, label=r'Final layer SVM ($\\lambda=1\\times10^{-13}$)')\n",
    "plt.colorbar(label='Training steps')\n",
    "\n",
    "#Plot end_to_end result\n",
    "data = result_end_to_end_df\n",
    "x = data['h']/data['P']\n",
    "y = data['test_loss']\n",
    "plt.scatter(x, y, c='k', label='Fully trained network ($\\lambda=0$)', marker='x')\n",
    "\n",
    "plt.xlabel('h/P')\n",
    "plt.ylabel(r'Test $\\mathcal{L}$')\n",
    "\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "plt.legend()\n",
    "# plt.ylim(0, 1)\n",
    "plt.axhline(np.min(data['test_loss']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why does the threshold $N_\\Delta/N = 1$ persist even throughout training?\n",
    "- maybe it doesn't, but the change in $N_{eff} \\ $ is linear rather than exponential, so it isn't showing up on the log-log plots?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot end_to_end result\n",
    "data = result_end_to_end_df\n",
    "x = data['P']/data['N_tilde']\n",
    "y = data['N_del']/data['N_tilde']\n",
    "plt.scatter(x, y, c='k')\n",
    "# plt.xscale('log')\n",
    "# plt.yscale('log')\n",
    "plt.ylim(0, .1)\n",
    "plt.xlim(1e-3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,6))\n",
    "data = result_df\n",
    "\n",
    "#invisible plot to set the limits correctly because matplotlib gets confused with log scale scatters\n",
    "x = data['N']\n",
    "y = data['N_del']\n",
    "\n",
    "plt.plot(x, y, color='none')\n",
    "plt.scatter(x, y, c=data['step'], cmap=cmap, norm=norm, alpha=.7, label=r'Final layer SVM ($\\lambda=1\\times10^{-13}$)')\n",
    "plt.colorbar(label='Training steps')\n",
    "\n",
    "#Plot end_to_end result\n",
    "data = result_end_to_end_df\n",
    "x = data['N']\n",
    "y = data['N_del']\n",
    "plt.scatter(x, y, c='k', label='Fully trained network ($\\lambda=0$)', marker='x')\n",
    "\n",
    "plt.xlabel('N')\n",
    "plt.ylabel(r'$N_{\\Delta}$')\n",
    "\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hessian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "untrained = result_df.query(\"step == @result_df['step'].min()\")\n",
    "trained = result_df.query(\"step == @result_df['step'].max()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 'h'\n",
    "y = 'train_loss'\n",
    "\n",
    "plt.scatter(untrained[x], untrained[y])\n",
    "plt.scatter(trained[x], trained[y])\n",
    "\n",
    "for step in sorted(result_df['step'].unique()):\n",
    "    df = result_df.query(\"step == @step\")\n",
    "    # Row with minimum value of N_del/N where train loss is non-zero and N_del/N >= 1 (underparameterized)\n",
    "    row = df.query('(train_loss > 1e-2)').sort_values('N').iloc[-1]\n",
    "    plt.scatter(row[x], row[y], c='k')\n",
    "    \n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "\n",
    "plt.xlabel(r'$N_\\Delta/N$')\n",
    "plt.ylabel('Train Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(untrained.eval('h/h_star'), untrained.eval('train_loss'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def symlog(x, thresh):\n",
    "    a = np.sign(x)\n",
    "    x_ = np.abs(x)\n",
    "    return np.where(x_ < thresh, x, a*(thresh + np.log10(x_/thresh)))\n",
    "\n",
    "def symexp(x, thresh):\n",
    "    a = np.sign(x)\n",
    "    x_ = np.abs(x)\n",
    "    return np.where(x_ < thresh, x, a*np.power(10, x_ - thresh)*thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sqrtbp(x):\n",
    "    a = np.sign(x)\n",
    "    x_ = np.abs(x)\n",
    "    return a*np.sqrt(x_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.scale import register_scale\n",
    "register_scale()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = untrained.query('h/h_star > 1e-1').query('h/h_star < 1.1').sort_values('h')[::3]\n",
    "thresh = 1e-5\n",
    "\n",
    "\n",
    "vals = data.eval('h/h_star')\n",
    "norm.autoscale(vals.values)\n",
    "norm.vmax = 1\n",
    "# norm.vmin = 1e-1\n",
    "sm = cm.ScalarMappable(norm=norm, cmap=cmap)\n",
    "\n",
    "for idx, row in data.iterrows():\n",
    "    eigs = np.sqrt(np.maximum(row['eigs0'], 0))\n",
    "    ax = sns.kdeplot(symlog(eigs, thresh), bw=.01, color=cmap(norm(vals[idx])), alpha=.8)\n",
    "    line = ax.get_lines()[-1]\n",
    "    x, y = line.get_data()\n",
    "    line.set_data(symexp(x, thresh), y)\n",
    "\n",
    "plt.ylim(0, 2.5)\n",
    "plt.colorbar(sm, label=r'$h/h^*$')\n",
    "plt.xlim(0, 500)\n",
    "plt.xscale('symlog', linthreshx=1e-2)\n",
    "             \n",
    "plt.xlabel(r'$\\sqrt{\\mu}$')\n",
    "plt.ylabel(r'$P\\left(\\sqrt{\\mu}\\right)$')\n",
    "plt.title(\"Hessian Spectrum (Random Features)\")\n",
    "plt.yscale('symlog', linthreshy=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = trained.query('h/h_star > 5e-1').query('h/h_star < 5').sort_values('h')#[::2]\n",
    "thresh = 1e-3\n",
    "\n",
    "\n",
    "vals = data.eval('h/h_star')\n",
    "norm.autoscale(vals.values)\n",
    "# norm.vmax = 2\n",
    "# norm.vmin = 1e-1\n",
    "sm = cm.ScalarMappable(norm=norm, cmap=cmap)\n",
    "\n",
    "for idx, row in data.iterrows():\n",
    "    eigs = row['eigs0']\n",
    "    pos_eigs = np.sqrt(eigs[eigs > 0])\n",
    "    ax = sns.kdeplot(symlog(pos_eigs, thresh), bw=.6/np.sqrt(len(pos_eigs)), color=cmap(norm(vals[idx])), alpha=.8)\n",
    "    line = ax.get_lines()[-1]\n",
    "    x, y = line.get_data()\n",
    "    x = symexp(x, thresh)\n",
    "    x = np.concatenate(([0, thresh/100], x))\n",
    "    gamma = float(len(eigs) - len(pos_eigs))/len(eigs)\n",
    "    y = np.concatenate(([gamma, 1], (1-gamma)*y))\n",
    "    line.set_data(x, y)\n",
    "\n",
    "plt.ylim(0, 2.5)\n",
    "plt.colorbar(sm, label=r'$h/h^*$')\n",
    "plt.xlim(0, 500)\n",
    "plt.xscale('symlog', linthreshx=1e-3)\n",
    "             \n",
    "plt.xlabel(r'$\\sqrt{\\mu}$')\n",
    "plt.ylabel(r'$P\\left(\\sqrt{\\mu}\\right)$')\n",
    "plt.title(\"Hessian Spectrum (Trained Features)\")\n",
    "plt.yscale('symlog', linthreshy=1e-3)\n",
    "# plt.ylim(1e-1, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm.brg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = trained.query('h/h_star > 5e-1').query('h/h_star < 5').sort_values('h')[::2]\n",
    "thresh = 1e-3\n",
    "\n",
    "\n",
    "vals = data.eval('h/h_star')\n",
    "norm.autoscale(vals.values)\n",
    "# norm.vmax = 2\n",
    "# norm.vmin = 1e-1\n",
    "sm = cm.ScalarMappable(norm=norm, cmap=cm.brg)\n",
    "\n",
    "for idx, row in data.iterrows():\n",
    "    print(1/np.sqrt(len(eigs)),)\n",
    "    eigs = sqrtbp(row['eigs0'])\n",
    "    ax = sns.kdeplot(symlog(eigs, thresh), bw=1/np.sqrt(len(eigs)), color=cm.brg(norm(vals[idx])), alpha=.8)\n",
    "    line = ax.get_lines()[-1]\n",
    "    x, y = line.get_data()\n",
    "    line.set_data(symexp(x, thresh), y)\n",
    "\n",
    "plt.ylim(0, 2)\n",
    "plt.colorbar(sm, label=r'$h/h^*$')\n",
    "plt.xlim(-10*thresh, 500)\n",
    "plt.xscale('symlog', linthreshx=100*thresh)\n",
    "             \n",
    "plt.xlabel(r'$\\sqrt{\\mu}$')\n",
    "plt.ylabel(r'$P\\left(\\sqrt{\\mu}\\right)$')\n",
    "plt.title(\"Hessian Spectrum (Trained Features)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:thesis]",
   "language": "python",
   "name": "conda-env-thesis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
