\documentclass[a4paper, 12pt, titlepage]{article}
\usepackage[margin=2.0cm]{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{lipsum}

\usepackage[style=ieee, url=false, doi=false, isbn=false]{biblatex}
\addbibresource{bibliography.bib}

\begin{document}

\title{\bf Random Features and the Jamming Transition in Artificial Neural Networks}
\author{
    % Hudson Cooper\\ % REDACT FOR SUBMISSION
    King's College London\\
    M.Sc.\ in Complex Systems Modelling
}
\date{\today}
\maketitle

\begin{abstract}
    Hello, world!\\
    TODO
\end{abstract}

\section{Introduction}
Neural networks are one of the most highly successful classes of models in modern machine and statistical learning. While there is a rich body of literature empirically examining neural networks and their ability to fit to and generalize from data, the field is lacking a cohesive theoretical framework for designing these networks and for understanding their incredible success.



\section{Background}

\subsection{The Jamming Transition: Loss Landscape and Generalization}
\cite{baity-jesiComparingDynamicsDeep2019}
\subsection{Random Features, Kernel Methods, and Deep Learning}
\subsection{Double Descent and Random Features}
\subsection{Feature Learning}

\section{Methods and Results}
\subsection{}

\section{Discussion}

\printbibliography
\end{document}