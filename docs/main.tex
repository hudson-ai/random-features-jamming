\documentclass[a4paper, 12pt, titlepage]{article}
\usepackage[margin=2.0cm]{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{lipsum}

\usepackage[style=ieee, url=false, doi=false, isbn=false]{biblatex}
\addbibresource{bibliography.bib}

\begin{document}

\title{\bf Random Features and the Jamming Transition in Artificial Neural Networks}
\author{
    % Hudson Cooper\\ % REDACT FOR SUBMISSION
    King's College London\\
    M.Sc.\ in Complex Systems Modelling
}
\date{\today}
\maketitle

\begin{abstract}
\lipsum[1]
\end{abstract}

\section{Introduction}
Neural networks are one of the most highly successful classes of models in modern machine and statistical learning. While there is a rich body of literature which empirically examines the ability of neural networks to fit to and generalize from data, the field is lacking a cohesive theoretical framework for designing these networks and for understanding their incredible success. \\

\subsection{Motivation}

One of the most baffling features of modern neural networks is that they operate in a regime that is at odds with a main tenet of classical statistical learning, the bias-variance trade-off. The bias-variance trade-off implies that more expressive models are likely to find spurious patterns in data, and therefore practitioners should seek to build models that are ``as simple as possible, but no simpler." Neural networks, however, typically operate in an extremely over-parameterized regime, containing many more parameters than there are data. Neural networks have been shown to be so highly expressive that they are able to interpolate data and perfectly classify training data, even when labels have been replaced with pure noise \cite{zhangUnderstandingDeepLearning2017}. Despite their complexity and extreme expressiveness, they are able to generalize extremely well in practice, often outperforming classical (under-parameterized) models on test data. \\

Recent work \cite{belkinReconcilingModernMachine2019} has characterised this phenomena in terms of the so-called `double-descent' curve, in which test (generalization) error as a function of model complexity follows a U-shaped curve, as is predicted by the classical bias-variance trade-off. The classical description holds up to the interpolation threshold, where models are able to obtain perfect performance on training data at the cost of extremely high test error. Beyond this threshold, however, the test error again decreases, and the global optimum often occurs deep in the over-parameterized regime at the point of `infinite' complexity. This behavior is far from unique to neural networks and has been empirically shown to occur in other contexts, for example in random forests and random feature models \cite{belkinUnderstandDeepLearning2018, belkinReconcilingModernMachine2019}.\\

Study of the training dynamics of neural networks undergoing stochastic gradient descent (SGD) in \cite{baity-jesiComparingDynamicsDeep2019} and \cite{geigerJammingTransitionParadigm2019} has shown that the interpolation threshold marks a phase transition between two different dynamical regimes, corresponding to the under- and over-parameterized regimes described in the double-descent literature. The over-parameterized regime is characterized by the ability of a neural network to achieve arbitrarily low loss on the training set, but because loss functions are generally not convex, it is not obvious what the necessary conditions are to guarantee that gradient descent can fit well to the training data. By appealing to analogies between the physical phenomena of jamming in disordered solids and the phase transition observed at the interpolation threshold in neural networks, \cite{geigerJammingTransitionParadigm2019} and \cite{spiglerJammingTransitionOverparametrization2019} predict and empirically test a characterization of the training dynamics near and beyond the threshold. 

\subsection{Goals and Roadmap}
The existing literature equates the jamming transition to the interpolation threshold and the corresponding cusp in test error that characterises the double-descent curve. However, random features models appear to recover the double-descent structure without displaying the phenomenology of elliptical jamming.

The aim of this paper is to explore the extent to which random features models recover the qualitative behaviors described in the jamming literature and to examine the role of feature learning in the jamming and double-descent phenomenon. 

\section{Background}

\subsection{The Jamming Transition: Loss Landscape and Generalization}
\subsection{Random Features, Kernel Methods, and Deep Learning}
\subsection{Double Descent and Random Features}
\subsection{Feature Learning}

\section{Methods and Results}
\section{Discussion}

\printbibliography
\end{document}