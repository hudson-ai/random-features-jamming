## Where I am


  -  Random features (RF) models with NTK features correspond to the lazy regime of DNN training, and they display the "double descent" curve. Feature learning seems to change the specifics a bit, e.g whether the global minimum is to the left or to the right of the cusp, etc., but the basic phenomenology seems to be the same.

  -  Spigler, Geiger, et. al. make some pretty specific predictions (and matching observations in their experiments) about the characteristics of the jamming transition (which seems to be identified with the cusp in the double descent curve) in real DNNs in the context of classification. They in particular look at the hinge loss, since the hinge loss can be thought of as a constraint satisfaction problem or set of particles with finite-range interactions. They make some predictions about the eigenvalues of the hessian in the vicinity of this transition, but another strong prediction is that there is a discontinuous jump in the number of unsatisfied constraints at the critical point.

  -  I have done some initial simulations that seem to show a cusp/double descent curve when using random features with the hinge loss (which I am solving with an SVM solver with varying amounts of regularization), which I can reproduce using NTK features or CK (conjugate kernel, i.e. training the last linear layer of a network) features. I also observe a cusp when using random (Gaussian) data with labels generated from a logistic model like the one I discussed in the pdf you just responded to.

  -  It stands to reason that we should see the same discontinuous jump in the number of unsatisfied constraints when we are at the generalization cusp (well, only *actually* discontinuous in the zero regularization limit) . I think this might be a good starting point. I have done some initial work on this, and so far I don't see any evidence of a discontinuity, which is pretty interesting. Everything is a bit noisy though, so I think I should try again with a larger data set and with more varying amounts of regularization.
      - I actually emailed the author of the SVM paper, and he gave me some pointers on how he went about numerically solving the equations in his paper, which should give some predictions for the number of unsatisfied constraints. He used the non-squared hinge loss, but I can redo my simulations in that context too. This could be a good way to get some more evidence of the existence or non-existence of the discontinuity i when regularization goes to zero without having to do a whole lot of my own theory. This should maybe go in the "if there is time later" pile too.

  - I think this is kind of a nice direction because it is interesting whether or not my results are null!
      - If there is a discontinuity, then this is some evidence that RF models not only show double descent, but they also show some other features of jamming. I could also take a look at the loss Hessian near the cusp in this case.
      - If there isn't a discontinuity, this is some evidence that the discontinuity and the jamming transition are properties of feature learning that coincide with *but are independent of* double descent, which can be reproduced with RF alone

# The beginning of a write-up


## Jamming and double descent
There are two interesting phenomena that occur when we move between under- and over-parameterized neural networks: "jamming" and "double descent". It isn't clear if these phenomena are exactly the same or if they are distinct but somehow related.

In the context of learning deep neural networks (DNNs) using stochastic gradient desceont (SGD), [@baity-jesiComparingDynamicsDeep2019] characterized distinct training dynamics in the "overparameterized" (many more parameters than data) and "underparameterized" (many more data than parameters) regimes. The underparameterized regime has a rough loss landscape, giving rise to glassy learning dynamics and poor learning abilities. The overparameterized region, on the other hand, has to a smooth loss landscape in which global maxima are easy to achieve. The authors conjectured a phase transition between these two phases, sharing some similarities to the "jamming transition" seen in disordered solids. In particular, the transition corresponds to the emergence of many flat directions in the loss/energy landscapes

By choosing a specific form for the loss function (namely, the quadratic hinge loss) in the context of classification, [@spiglerJammingTransitionOverparametrization2019] re-casts DNN training as a constraint satisfaction problem, allowing deeper analogy between the over/underparametrerization transition and jamming. In particular, for a fixed number of data $P$ and various values of the number of parameters $N$, the authors examined the number of unsatisfied constraints $N_\Delta$, i.e. the number of data-points contributing non-zero quantities to the loss function, at the end of training. By appealing to

They observed a discontinuous jump in $N_\Delta/N$ as a function of $P/N$

 the role of the number of constraints as a fraction of the number of degrees of freedom  [@geigerJammingTransitionParadigm2019]


## Ideas:

1. Train last layer with
- a random initialization
- a partially trained first layer
- a mostly trained first layer
- etc. (logarithmically spaced? features seem to emerge on exponential time-scales)

2. In double trouble, they found two contributions to the cusp:
- noise in training data
- noise in initialization

  - They use a teacher-student model:
    - idea: initialize student closer to teacher (cheating a bit!)
      - interpolate between perfect initialization and ransom initialization
      - probe role of initialization of random feature layer


## Citations
